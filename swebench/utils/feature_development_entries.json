[
    {
        "instance_id": "tokio-rs__tokio-6724",
        "problem_statement": "Vectored IO for `write_all_buf`\n**Is your feature request related to a problem? Please describe.**\r\n\r\nThe `AsyncWriteExt` trait provides the `write_all_buf` function to write the entire contents of a `Buf` type to the underlying writer. However, if the buf is fragmented (eg a VecDeque<u8> or Chain), then it can have potentially bad performance with the current implementation, writing many small buffers at a time. This is because the current impl only uses `chunk()` to get the first chunk slice only.\r\n\r\nhttps://github.com/tokio-rs/tokio/blob/a02407171a3f1aeb86e7406bcac9dfb415278308/tokio/src/io/util/write_all_buf.rs#L47\r\n\r\n**Describe the solution you'd like**\r\n\r\nIf the underlying writer `is_write_vectored()`, `write_all_buf` could make use of `Buf::chunks_vectored` to fill an IO slice to use with `poll_write_vectored`.\r\n\r\nThe vectored io-slice can use a fixed size array, eg 4 or 8. When advancing the io-slice, should a chunk be removed, it could call `chunks_vectored` again to fill the io-slice, considering that chunks_vectored should be a fairly cheap operation.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nSimilar implementation discussions have occurred in #3679.\r\nPerformance testing is needed, and real-world use cases of `write_all_buf` should be examined\r\n\r\n\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "tokio-rs__tokio-6593",
        "problem_statement": "Allow setting `unhandled_panic` behavior as option on `tokio::test`\n**Is your feature request related to a problem? Please describe.**\r\nI have several unit tests that run some handler code that is under test in a `tokio::spawn`ed task, and sends/receives bytes to/from that handler code from the main task. My AsyncRead + AsyncWrite mock will panic if it sees unexpected bytes, and if this happens in the background task the test will hang. I'd prefer the test to shut down in this scenario, and so I'm using the `unhandled_panic` option introduced by #4516.\r\n\r\n**Describe the solution you'd like**\r\n`#[tokio::test(unhandled_panic = ShutdownRuntime)`\r\n\r\n**Describe alternatives you've considered**\r\nCurrently I manually set up a tokio runtime for my tests that require this behavior.\r\n\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "tokio-rs__tokio-3441",
        "problem_statement": "Standardize missing executor panic messages\nSometimes you get an error like:\r\n\r\n> there is no reactor running, must be called from the context of Tokio runtime\r\n\r\nSometimes it's like:\r\n\r\n> not currently running on the Tokio runtime\r\n\r\n**Describe the solution you'd like**\r\n\r\nIdeally, these would all have the same error text, or at least a common substring. This makes searching for and finding solutions easier.\r\n\r\n**Additional context**\r\n\r\n[Why do I get the error “there is no reactor running, must be called from the context of Tokio runtime” even though I have #[tokio::main]?](https://stackoverflow.com/a/64780012/155423)\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "tokio-rs__tokio-2145",
        "problem_statement": "Improve panic messages when runtime is not running\nThe panic messages when the reactor or timer are not available should be improved to better explain what a user needs to do to fix them (originally reported at https://github.com/hyperium/hyper/issues/1995).\r\n\r\n- Current timer panic: `timer error: timer is shutdown`\r\n- Current reactor panic: `no current reactor`\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "asterinas__asterinas-1125",
        "problem_statement": "Lockless mutability for current task data.\n**This is currently a work-in-progress RFC**\r\n\r\n<!-- Thank you for taking the time to propose a new idea or significant change. Please provide a comprehensive overview of the concepts and motivations at play. -->\r\n\r\n### Summary\r\n\r\n<!-- Briefly summarize the idea, change, or feature you are proposing. What is it about, and what does it aim to achieve? -->\r\n\r\nThis RFC plans to introduce a mechanism for implementing lock-less inner mutability of task data that would be only accessible through the current task.\r\n\r\n### Context and Problem Statement\r\n\r\n<!-- Describe the problem or inadequacy of the current situation/state that your proposal is addressing. This is a key aspect of putting your RFC into context. -->\r\n\r\nIn `aster-nix`, there would be a hell lot of inner mutability patterns using `Mutex` or `SpinLock` in the thread structures, such as `SigMask`, `SigStack` and `sig_context`, etc. They are all implemented with locks. However, they should only be accessed through the current thread. There would be no syncing required. Modifying them from non-current threads should be illegal. Locks are too heavy-weighted for such kind of inner mutability patterns.\r\n\r\nAlso, for shared thread/task data, we access them using `current!` in thread/task contexts. These operations would also require fetching the task from a `cpu_local!` object that incurs heavy type-checking and interrupt/preempt blocking operations. Such jobs can be ignored when the caller is definitely in the current task's contexts. As #1105 points out, even the most simple system call `getpid` would access such task exclusive variables many times. Current implementation would require multiple locking operations and IRQ/preempt guarding operations. Many cycles are wasted doing so.\r\n\r\nWe currently only have per-task data storage that is shared (the implementer should provide `Send + Sync` types). Most of the data that don't need to be shared are also stored here, which would require a lock for inner mutability. In this RFC, I would like to introduce a new kind of data in the `ostd::Task` that is exclusive (not shared, no need to be `Send + Sync`). It would offer a chance to implement the above mentioned per-task storage without locks, boosting the performance by a lot.\r\n\r\n### Proposal\r\n\r\nCurrently we access them via `current!()`, which would return a reference over the current task and it's corresponding data. The data is defined within a structure (either `PosixThread` or `KernelThread` currently).\r\n\r\nIn `aster-nix`, most code are running in the context of a task (other code runs in interrupt contexts). So the code would only have one replica of task local exclusive data that is accessible. Such data would only be accessed by the code in the corresponding task context also. Such kind of data should be safely mutably accessed. OSTD should provide a way to define task-context-global per-task mutable variables that are not visible in interrupt contexts. By doing so, many of the data specific to a task can be implemented lock-less.\r\n\r\n<!-- Clearly and comprehensively describe your proposal including high-level technical specifics, any new interfaces or APIs, and how it should integrate into the existing system. -->\r\n\r\n#### Task entry point\r\n\r\nThe optimal solution would let the task function receive references to the task data as arguments. Then all the functions that requires the data of the current task would like to receive arguments like so. This is the requirement of a function that should be used as a task entry point:\r\n\r\n```rust\r\n/// The entrypoint function of a task takes 4 arguments:\r\n///  1. the mutable task context,\r\n///  2. the shared task context,\r\n///  3. the reference to the mutable per-task data,\r\n///  4. and the reference to the per-task data.\r\npub trait TaskFn =\r\n    Fn(&mut MutTaskInfo, &SharedTaskInfo, &mut dyn Any, &(dyn Any + Send + Sync)) + 'static;\r\n```\r\n\r\nAn example of usage:\r\n\r\n```rust\r\n// In `aster-nix`\r\n\r\nuse ostd::task::{MutTaskInfo, Priority, SharedTaskInfo};\r\nuse crate::thread::{\r\n    MutKernelThreadInfo, MutThreadInfo, SharedKernelThreadInfo, SharedThreadInfo, ThreadExt,\r\n};\r\n\r\nfn init_thread(\r\n    task_ctx_mut: &mut MutTaskInfo,\r\n    task_ctx: &SharedTaskInfo,\r\n    thread_ctx_mut: &mut MutThreadInfo,\r\n    thread_ctx: &SharedThreadInfo,\r\n    kthread_ctx_mut: &mut MutKernelThreadInfo,\r\n    kthread_ctx: &SharedKernelThreadInfo,\r\n) {\r\n    println!(\r\n        \"[kernel] Spawn init thread, tid = {}\",\r\n        thread_ctx.tid\r\n    );\r\n    let initproc = Process::spawn_user_process(\r\n        karg.get_initproc_path().unwrap(),\r\n        karg.get_initproc_argv().to_vec(),\r\n        karg.get_initproc_envp().to_vec(),\r\n    )\r\n    .expect(\"Run init process failed.\");\r\n    // Wait till initproc become zombie.\r\n    while !initproc.is_zombie() {\r\n        // We don't have preemptive scheduler now.\r\n        // The long running init thread should yield its own execution to allow other tasks to go on.\r\n        task_ctx_mut.yield_now();\r\n    }\r\n}\r\n\r\n#[controlled]\r\npub fn run_first_process() -> ! {\r\n    let _thread = thread::new_kernel(init_thread, Priority::normal(), CpuSet::new_full());\r\n}\r\n```\r\n\r\nSuch approach can eliminate the need of neither `current!` nor `current_thread!`, but introduces verbose parameters for the functions. This approach would be implemented by #1108 .\r\n\r\n### Motivation and Rationale\r\n\r\n<!-- Elaborate on why this proposal is important. Provide justifications for why it should be considered and what benefits it brings. Include use cases, user stories, and pain points it intends to solve. -->\r\n\r\n### Detailed Design\r\n\r\n<!-- Dive into the nitty-gritty details of your proposal. Discuss possible implementation strategies, potential issues, and how the proposal would alter workflows, behaviors, or structures. Include pseudocode, diagrams, or mock-ups if possible. -->\r\n\r\n### Alternatives Considered\r\n\r\n<!-- Detail any alternative solutions or features you've considered. Why were they discarded in favor of this proposal? -->\r\n\r\n#### Context markers\r\n\r\nOf course, the easiest way to block IRQ code from accessing task exclusive local data is to have a global state `IN_INTERRUPT_CONTEXT` and check for this state every time when accessing the task exclusive local variables. This would incur some (but not much) runtime overhead. Such overhead can be eliminated by static analysis, which we would encourage.\r\n\r\nThere would be 3 kind of contexts: the bootstrap context, the task context and the interrupt context. So the code would have $2^3=8$ types of possibilities to run in different contexts. But there are only 4 types that are significant:\r\n\r\n 1. Utility code that could run in all 3 kind of contexts;\r\n 2. Bootstrap code that only runs in the bootstrap context;\r\n 3. The IRQ handler that would only run in the interrupt context;\r\n 4. Task code that would only run in the task context.\r\n\r\nOther code can be regarded as the type 1., since we do not know where would it run (for example, the page table cursor methods).\r\n\r\nCode must be written in functions (except for some really low level bootstrap code, which are all in OSTD). So we can mark functions with the above types, and check if type 1./2./3. functions accessed task local exclusive global variables.\r\n\r\nHere are the rules for function types:\r\n\r\n - All functions that may call 2. should be 2., the root of type 2. function is `ostd::main` and `ostd::ap_entry`;\r\n - all functions that may call 3. should be 3., the root of type 3. functions are send to `IrqLine::on_active`;\r\n - all functions that may call 4. should be 4., the root of type 4. functions are send to `TaskOptions`;\r\n - if a function can be call with multiple types of functions, it is type 1.\r\n\r\nIn this alternative, two tools will be introduced:\r\n\r\n 1. A procedural macro crate `code_context` (re-exported by OSTD) that provides function attributes `#[code_context::task]`, `#[code_context::interrupt]`, `#[code_context::boot]`. If not specified, the function is type 1.;\r\n 2. A tools that uses rustc to check the above rules ([an example](https://github.com/heinzelotto/rust-callgraph/tree/master)). OSDK would run this tool before compilation to reject unsound code.\r\n\r\n### Additional Information and Resources\r\n\r\n<!-- Offer any additional information, context, links, or resources that stakeholders might find helpful for understanding the proposal. -->\r\n\r\n### Open Questions\r\n\r\n<!-- List any questions that you have that might need further discussion. This can include areas where you are seeking feedback or require input to finalize decisions. -->\r\n\r\n### Future Possibilities\r\n\r\n<!-- If your RFC is likely to lead to subsequent changes, provide a brief outline of what those might be and how your proposal may lay the groundwork for them. -->\r\n\r\n<!-- We appreciate your effort in contributing to the evolution of our system and look forward to reviewing and discussing your ideas! -->\r\n\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "apache__arrow-rs-5439",
        "problem_statement": "Refine `Display` implementation for `FlightError`\n**Is your feature request related to a problem or challenge? Please describe what you are trying to do.**\r\n\r\nThere's a `TODO` for a better `std::fmt::Display` implementation on `FlightError`. Currently it forwards to `std::fmt::Debug`, which does not appear to be a good practice as errors should describe themselves with friendly messages provided by `Display`.\r\n\r\nhttps://github.com/apache/arrow-rs/blob/ef5c45cf4186a8124da5a1603ebdbc09ef9928fc/arrow-flight/src/error.rs#L50-L55\r\n\r\n**Describe the solution you'd like**\r\n\r\nMatch the variants of the error and specify different prompts like what we did for `ArrowError`.\r\n\r\nhttps://github.com/apache/arrow-rs/blob/ef5c45cf4186a8124da5a1603ebdbc09ef9928fc/arrow-schema/src/error.rs#L79-L87\r\n\r\n**Describe alternatives you've considered**\r\n\r\nDerive the implementation with `thiserror`. The code can be more concise with the cost of introducing a new build-time dependency.\r\n\r\n**Additional context**\r\n\r\nA better practice to implement `Display` for errors is **NOT** to include the error source. AWS SDK has adopted this as described in https://github.com/awslabs/aws-sdk-rust/issues/657. However, this could be considered as a breaking change as many developers have not realize that one should leverage something like [`std::error::Report`](https://doc.rust-lang.org/stable/std/error/struct.Report.html) to get the error sources printed.\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "apache__arrow-rs-5717",
        "problem_statement": "[parquet_derive] support OPTIONAL (def_level = 1) columns by default\n## Problem Description\r\n<!--\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...] \r\n(This section helps Arrow developers understand the context and *why* for this feature, in addition to  the *what*)\r\n-->\r\nI'm working on parquet files written by `pyarrow` (embedded in `pandas`). I came across `parquet_derive` and it avoids boilerplates in my project.\r\nThe problem is, it doesn't work on the parquet files that is written by `pandas` with default setup, it throws error information:\r\n\r\n```text\r\nParquet error: must specify definition levels\r\n```\r\n\r\nAfter digging into this, I found that the problem is the parquet file generated by `pyarrow` has def_level=1, i.e., every column, even without a null value, is OPTIONAL.\r\n\r\n<img width=\"677\" alt=\"image\" src=\"https://github.com/apache/arrow-rs/assets/27212391/b6b4cc96-8c53-4d41-9c66-4f802476dd7a\">\r\n\r\nHowever, the macro generate code that does not allow definition level, thus it fails to parsing columns with OPTIONAL value, even there is no actual NULL values:\r\n\r\n```rust\r\ntyped.read_records(num_records, None, None, &mut vals)?;\r\n```\r\n\r\nThe API it calls is: https://docs.rs/parquet/latest/parquet/column/reader/struct.GenericColumnReader.html#method.read_records .\r\n\r\n## My Solution\r\n\r\nThe solution is straight-forward. I have fixed the problem locally, I'm willing to contribute a pull request, but I don't know if this solution is reasonable in the scope of the whole `arrow` project.\r\n\r\nBasically, I think we need to provide definition level in `read_record`:\r\n\r\n```rust\r\ntyped.read_records(num_records, None /*should use a Some(&mut Vec<i16>)*/, None, &mut vals)?;\r\n```\r\n\r\nIn one word, with this solution, `parquet_derive` can now handle:\r\n1. (already supported) parquet file with all columns REQUIRED\r\n2. **(new introduced) parquet file with OPTIONAL columns but are always guaranteed to be valid**.\r\n\r\n### Pros\r\n\r\n- This solution does not break current features\r\n- This solution makes parquet_derive more general in handling parquet files.\r\n\r\nIt can pass the tests in `parquet_derive_tests`. I also add checks against the parsed records and valid records, to avoid abusing it for columns with NULLs.\r\n\r\n### Cons\r\n\r\n- It will be slightly slower since it allocates an extra `Vec<i16>` for each column when invoking `read_from_row_group`.\r\n\r\nI don't think it is a big deal, though, compared to the inconvenience of not supporting OPTIONAL columns. Moreover, we can make use of the max_def_levels (for REQUIRED column, it is 0) to skip creating the Vec.\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "apache__arrow-rs-6269",
        "problem_statement": "parquet_derive: support reading selected columns from parquet file\n# Feature Description\r\n\r\nI'm effectively using `parquet_derive` in my project, and I found that there are two inconvenient constraints:\r\n\r\n1. The `ParquetRecordReader` enforces the struct to organize fields exactly in the **same order** in the parquet file.\r\n2. The `ParquetRecordReader` enforces the struct to parse **all fields** in the parquet file. \"all\" might be exaggerating, but it is what happens if you want to get the last column, even only the last column.\r\n\r\nAs describe in its document:\r\n\r\n> Derive flat, simple RecordReader implementations. Works by parsing a struct tagged with #[derive(ParquetRecordReader)] and emitting the correct writing code for each field of the struct. Column readers are generated in the order they are defined.\r\n\r\nIn my use cases (and I believe these are common requests), user should be able to read pruned parquet file, and they should have the freedom to re-organize fields' ordering in decoded struct.\r\n\r\n# My Solution\r\n\r\nI introduced a `HashMap` to map field name to its index. Of course, it assumes field name is unique, and this is always true since the current `parquet_derive` macro is applied to a flat struct without nesting.\r\n\r\n# Pros and Cons\r\n\r\nObviously removing those two constraints makes `parquet_derive` a more handy tool.\r\n\r\nBut it has some implied changes:\r\n\r\n- previously, since the `ParquetRecordReader` relies only on the index of fields, it allows that a field is named as `abc` to implicitly rename itself to `bcd` in the encoded struct. After this change, user must guarantee that the field name in `ParquetRecordReader` to exist in parquet columns.\r\n  - I think it is more intuitive and more natural to constrain the \"field name\" rather than \"index\", if we use `ParquetRecordReader` to derive a decoder macro.\r\n- allowing reading partial parquet file may improve the performance for some users, but introducing a `HashMap` in the parser may slowdown the function a bit.\r\n  - when the `num_records` in a single parsing call is large enough, the cost of `HashMap` lookup is negligible.\r\n\r\nBoth implied changes seem to have a more positive impact than negative impact. Please review if this is a reasonable feature request. \r\n\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "",
        "problem_statement": "Support `peek_next_page` and `skip_next_page` in `InMemoryPageReader`\n**Is your feature request related to a problem or challenge? Please describe what you are trying to do.**\r\nwhen i was implementing bench using `skip_records` got\r\n\r\n```\r\nBenchmarking arrow_array_reader/Int32Array/binary packed skip, mandatory, no NULLs: Warming up for 3.0000 sthread 'main' panicked at 'not implemented', /CLionProjects/github/arrow-rs/parquet/src/util/test_common/page_util.rs:169:9\r\n\r\n```\r\n\r\nwhich is unimplemented\r\n\r\n**Describe the solution you'd like**\r\n<!--\r\nA clear and concise description of what you want to happen.\r\n-->\r\n\r\n**Describe alternatives you've considered**\r\n<!--\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n-->\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context or screenshots about the feature request here.\r\n-->\r\n\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "apache__arrow-rs-4909",
        "problem_statement": "Add read access to settings in `csv::WriterBuilder`\n**Is your feature request related to a problem or challenge? Please describe what you are trying to do.**\r\n\r\nWhile implementing CSV writing in DataFusion (see https://github.com/apache/arrow-datafusion/pull/7390/files), we would like to be able to check the value of `has_headers` before actually constructing a csv writer. However, the current API has no way to do read the current values (only modify them): https://docs.rs/arrow-csv/45.0.0/arrow_csv/writer/struct.WriterBuilder.html\r\n\r\n\r\n**Describe the solution you'd like**\r\nIt would be nice to have read only access to the fields. Maybe something like\r\n\r\n```rust\r\n    let builder = WriterBuilder::new().has_headers(false);\r\n \r\n    let has_headers = builder.get_has_headers()\r\n```\r\n\r\nIt is somewhat unfortunate that the builder already uses `has_headers` to set the field names rather than `with_has_headers`\r\n\r\n**Describe alternatives you've considered**\r\nWe can keep a copy of has_headers around as @devinjdangelo  has done in https://github.com/apache/arrow-datafusion/pull/7390/files\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context or screenshots about the feature request here.\r\n-->\r\n\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "apache__arrow-rs-2044",
        "problem_statement": "Support `peek_next_page()` and `skip_next_page` in `SerializedPageReader`\n**Is your feature request related to a problem or challenge? Please describe what you are trying to do.**\r\nAdd `skip_next_page` and `peek_next_page` function to SerializedPageReader that uses the column index to skip the next page without reading it.\r\nrelated #1792 \r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "GuillaumeGomez__sysinfo-245",
        "problem_statement": "Feature Request: support retrieve CPU number and load info\nThanks for providing the awesome library for retrieving system information. But some information cannot be retrieved by this crate, like CPU number and CPU average load. (So I must use another crate like https://docs.rs/sys-info/0.5.8/sys_info/index.html).\r\n\r\nIf this crate can provide a full feature, it's will be great.\r\n\r\nThanks to all authors and contributors for this repo.\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "GuillaumeGomez__sysinfo-679",
        "problem_statement": "Add check to ensure that types are using common md files and not manual doc comments\nFor example `System` or `Process`.\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "GuillaumeGomez__sysinfo-1161",
        "problem_statement": "Should `exe` be included in default process retrieval (and removed from `ProcessRefreshKind`)?\nIt seems to be a basic information that everyone might want all the time.\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "crossbeam-rs__crossbeam-454",
        "problem_statement": "AtomicCell without lock for non-usize values in stable.\nNow that `std::atomic::AtomicU{8,16,32,64}` has been stabilized, please allow `AtomicCell` to store values with non-`usize` widths without a lock. \r\n\r\nThis would require increasing the min rust version. Let me know if you like me to send a pull request.\r\n\r\nThanks.\n",
        "response": "Feature Development"
    },
    {
        "instance_id": "dtolnay__syn-1714",
        "problem_statement": "Parse explicit tail call syntax\nhttps://github.com/rust-lang/rust/pull/112887\n",
        "response": "Feature Development"
    }
]