{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets,Features,Sequence,Value\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "import re\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    \"repo\": Value(\"string\"),\n",
    "    \"pull_number\": Value(\"int64\"),\n",
    "    \"test_patch\": Value(\"string\"),\n",
    "    \"issue_numbers\": Sequence(Value(\"string\")),\n",
    "    \"instance_id\": Value(\"string\"),\n",
    "    \"problem_statement\": Value(\"string\"),\n",
    "    \"version\": Value(\"string\"),\n",
    "    \"base_commit\": Value(\"string\"),\n",
    "    \"patch\": Value(\"string\"),\n",
    "    \"created_at\": Value(\"string\"),\n",
    "    \"hints_text\": Value(\"string\"),\n",
    "    \"environment_setup_commit\": Value(\"string\"),\n",
    "    \"FAIL_TO_PASS\": Sequence(Value(\"string\")),\n",
    "    \"PASS_TO_PASS\": Sequence(Value(\"string\")),\n",
    "    \"FAIL_TO_FAIL\": Sequence(Value(\"string\")),  # 显式定义为字符串数组\n",
    "    \"PASS_TO_FAIL\": Sequence(Value(\"string\")),  # 显式定义为字符串数组\n",
    "})\n",
    "args = Namespace(\n",
    "    dataset_name=\"r1v3r/RustGPT_Bench_verified\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "if not token:\n",
    "    print(\"Error: Hugging Face access token not provided. Use --token or set the HUGGINGFACE_TOKEN environment variable.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "login(token=token)\n",
    "\n",
    "# Load the dataset\n",
    "print(f\"Loading dataset '{args.dataset_name}' split '{args.split}' from Hugging Face...\")\n",
    "try:\n",
    "    dataset = load_dataset(args.dataset_name, split=args.split)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "dataset =  dataset.remove_columns('r_number')\n",
    "print(\"After removing r_number:\", dataset.column_names)\n",
    "dataset.cast(features)\n",
    "dataset.push_to_hub(args.dataset_name, split=args.split,token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast(features)\n",
    "\n",
    "hyper_dataset = load_dataset(\"r1v3r/hyper_validated\", split=\"train\")\n",
    "hyper_dataset = hyper_dataset.cast(features)\n",
    "# 筛选数据\n",
    "hyper_dataset = hyper_dataset.filter(\n",
    "    lambda x: x[\"instance_id\"] == \"hyperium__hyper-3261\" or x[\"instance_id\"] == \"hyperium__hyper-3275\"\n",
    ")\n",
    "\n",
    "# print(hyper_dataset)\n",
    "serde_dataset = load_dataset(\"r1v3r/serde_validated\", split=\"train\",features=features)\n",
    "serde_dataset = serde_dataset.cast(features)\n",
    "proc_macro2_dataset = load_dataset(\"r1v3r/proc-macro2_validated\", split=\"train\",features=features)\n",
    "proc_macro2_dataset = proc_macro2_dataset.cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "temp = concatenate_datasets([hyper_dataset, serde_dataset, proc_macro2_dataset ,dataset])\n",
    "\n",
    "temp.push_to_hub(\"r1v3r/RustGPT_Bench_100\", token=token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改某个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = None\n",
    "for i, instance in enumerate(dataset):\n",
    "    if instance.get(\"instance_id\") == \"rayon-rs__rayon-986\":\n",
    "        index = i\n",
    "        break\n",
    "\n",
    "if index is not None:\n",
    "    # Update the 'pull_number' for the found instance\n",
    "    dataset = dataset.map(lambda example, idx: \n",
    "                          {**example,\"pull_number\": 986} if idx == index else example, \n",
    "                          with_indices=True)\n",
    "    print(f\"Updated 'pull_number' to 2 for instance with 'instance_id' 1.\")\n",
    "else:\n",
    "    print(\"Error: No instance found with 'instance_id' == 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计buggy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_patch(example):\n",
    "    patch_text = example['patch']\n",
    "    # Use regex to find all lines that indicate file changes\n",
    "    file_changes = re.findall(r'(?m)^(--- |\\+\\+\\+ )([^\\s]+)', patch_text)\n",
    "    # Extract unique file paths\n",
    "    unique_files = set(file for _, file in file_changes)\n",
    "    # Return a dictionary with instance_id and the count of unique files\n",
    "    return {'instance_id': example['instance_id'], 'file_number': len(unique_files)}\n",
    "\n",
    "# Map the function over the dataset\n",
    "results = dataset.map(count_files_in_patch, remove_columns=dataset.column_names)\n",
    "\n",
    "# Convert the results to a dictionary for easy lookup\n",
    "instance_file_map = {result['instance_id']: result['file_number'] for result in results}\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for instance_id, file_number in instance_file_map.items():\n",
    "    print(f\"{instance_id}: {file_number}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问gpt哪些是关于feature的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 648/648 [15:14<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: This problem statement can be categorized as a **Bug Fix**. The issues described involve correcting the behavior of existing functions (`split()` and `splitn()`) that are not returning the expected results, which indicates that there are bugs in the implementation that need to be fixed. Additionally, the mention of existing tests passing incorrectly further supports this classification.\n",
      "Saving Feature Development entries to feature_development_entries.json...\n",
      "分类完成，结果已保存。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json  # 用于保存 JSON 文件\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 配置 OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.base_url = \"https://api5.xhub.chat/v1/\"\n",
    "dataset = load_dataset(\"r1v3r/auto_0207\", split=\"train\")\n",
    "# 模型配置\n",
    "model_name_or_path = \"gpt-4o-mini\"  # 或者 \"gpt-3.5-turbo\"\n",
    "system_messages = \"You are an AI assistant that categorizes tasks into 'Bug Fix' or 'Feature Development'.\"\n",
    "temperature = 0.5\n",
    "top_p = 0.9\n",
    "\n",
    "def classify_problem_statement(problem_statement):\n",
    "    \"\"\"\n",
    "    使用 OpenAI API 对 problem_statement 进行分类。\n",
    "\n",
    "    Args:\n",
    "        problem_statement (str): 问题陈述。\n",
    "\n",
    "    Returns:\n",
    "        str: 分类结果，可能的值为 'Bug Fix', 'Feature Development', 'Unknown', 'Error'。\n",
    "    \"\"\"\n",
    "    user_message = (\n",
    "        f\"Categorize the following problem statement as either 'Bug Fix' or 'Feature Development':\\n\\n\\\"{problem_statement}\\\"\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model_name_or_path,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_messages},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "\n",
    "        # 根据 OpenAI API 的响应格式提取分类结果\n",
    "        classification = response.choices[0].message.content.strip()\n",
    "        print(f\"Classification: {classification}\")\n",
    "\n",
    "        # 确保分类结果为预期的值\n",
    "        if 'Bug Fix' in classification:\n",
    "            return 'Bug Fix'\n",
    "        elif 'Feature Development' in classification:\n",
    "            return 'Feature Development'\n",
    "        else:\n",
    "            return 'Unknown'  # 无法确定的情况\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing problem_statement: {problem_statement}\\nError: {e}\")\n",
    "        return 'Error'\n",
    "\n",
    "\"\"\"\n",
    "主函数，用于加载数据集，分类 problem_statement,并保存结果。\n",
    "\"\"\"\n",
    "# 加载数据集\n",
    "# 初始化一个列表，用于存储分类为 'Feature Development' 的条目\n",
    "feature_development_entries = []\n",
    "\n",
    "# 迭代数据集中的每一项\n",
    "print(\"Classifying problem statements...\")\n",
    "for example in tqdm(dataset, desc=\"Classifying\"):\n",
    "    problem_statement = example.get('problem_statement', None)\n",
    "    instance_id = example.get('instance_id', None)  # 确保 'instance_id' 字段存在\n",
    "\n",
    "    if pd.isna(problem_statement):\n",
    "        classification = 'Unknown'\n",
    "    else:\n",
    "        classification = classify_problem_statement(problem_statement)\n",
    "\n",
    "    # 如果分类为 'Feature Development'，将相关信息添加到列表中\n",
    "    if classification == 'Feature Development':\n",
    "        entry = {\n",
    "            \"instance_id\": instance_id,\n",
    "            \"problem_statement\": problem_statement,\n",
    "            \"response\": classification\n",
    "        }\n",
    "        print(\"instance_id:\", instance_id)\n",
    "        feature_development_entries.append(entry)\n",
    "\n",
    "# 保存分类为 'Feature Development' 的条目到 JSON 文件\n",
    "output_json_path = \"feature_development_entries.json\"  # 指定输出 JSON 文件的路径\n",
    "print(f\"Saving Feature Development entries to {output_json_path}...\")\n",
    "with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(feature_development_entries, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"分类完成，结果已保存。\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去除feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data has been saved to /home/riv3r/SWE-bench/swebench/utils/ipynb/output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 假设 json1 和 json2 是文件路径\n",
    "json1_path = '/home/riv3r/SWE-bench/swebench/utils/merged_dataset.json'\n",
    "json2_path = '/home/riv3r/SWE-bench/feature_development_entries.json'\n",
    "output_jsonl_path = '/home/riv3r/SWE-bench/swebench/utils/ipynb/output.json'  # 过滤后的输出文件路径\n",
    "\n",
    "# 加载 json2 文件的数据并提取所有的 instance_id\n",
    "with open(json2_path, 'r', encoding='utf-8') as file:\n",
    "    json2_data = json.load(file)\n",
    "\n",
    "json2_instance_ids = {item['instance_id'] for item in json2_data}\n",
    "\n",
    "# 打开 json1 文件进行逐行读取，并打开输出文件准备写入\n",
    "with open(json1_path, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    for line in infile:\n",
    "        try:\n",
    "            # 解析每一行的 JSON 数据\n",
    "            item = json.loads(line.strip())\n",
    "            \n",
    "            # 检查当前项的 instance_id 是否不在 json2 的 instance_id 列表中\n",
    "            if item.get('instance_id') not in json2_instance_ids:\n",
    "                # 将符合条件的项写入输出文件\n",
    "                outfile.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"无法解析的 JSON 行: {line}, 错误: {e}\")\n",
    "\n",
    "print(f\"Filtered data has been saved to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = args.dataset_name\n",
    "print(f\"Uploading the cleaned dataset to Hugging Face repository '{repo_id}'...\")\n",
    "\n",
    "dataset.push_to_hub(repo_id, token=token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
