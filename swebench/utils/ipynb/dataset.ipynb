{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets,Features,Sequence,Value\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "import re\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'r1v3r/RustGPT_Bench_verified' split 'train' from Hugging Face...\n",
      "After removing r_number: ['repo', 'pull_number', 'instance_id', 'issue_numbers', 'base_commit', 'patch', 'test_patch', 'problem_statement', 'hints_text', 'created_at', 'version', 'environment_setup_commit', 'FAIL_TO_PASS', 'PASS_TO_PASS', 'FAIL_TO_FAIL', 'PASS_TO_FAIL']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91e4e9fccdc4a99953a232e4552adf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2bf2d8e6224fbdbb41d6b0ea0ec0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2069af2087464581f5de61817b17c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/r1v3r/RustGPT_Bench_verified/commit/b54f053c0ccb52f32cc5ffd13bb064fcc00ea18e', commit_message='Upload dataset', commit_description='', oid='b54f053c0ccb52f32cc5ffd13bb064fcc00ea18e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/r1v3r/RustGPT_Bench_verified', endpoint='https://huggingface.co', repo_type='dataset', repo_id='r1v3r/RustGPT_Bench_verified'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = Features({\n",
    "    \"repo\": Value(\"string\"),\n",
    "    \"pull_number\": Value(\"int64\"),\n",
    "    \"test_patch\": Value(\"string\"),\n",
    "    \"issue_numbers\": Sequence(Value(\"string\")),\n",
    "    \"instance_id\": Value(\"string\"),\n",
    "    \"problem_statement\": Value(\"string\"),\n",
    "    \"version\": Value(\"string\"),\n",
    "    \"base_commit\": Value(\"string\"),\n",
    "    \"patch\": Value(\"string\"),\n",
    "    \"created_at\": Value(\"string\"),\n",
    "    \"hints_text\": Value(\"string\"),\n",
    "    \"environment_setup_commit\": Value(\"string\"),\n",
    "    \"FAIL_TO_PASS\": Sequence(Value(\"string\")),\n",
    "    \"PASS_TO_PASS\": Sequence(Value(\"string\")),\n",
    "    \"FAIL_TO_FAIL\": Sequence(Value(\"string\")),  # 显式定义为字符串数组\n",
    "    \"PASS_TO_FAIL\": Sequence(Value(\"string\")),  # 显式定义为字符串数组\n",
    "})\n",
    "args = Namespace(\n",
    "    dataset_name=\"r1v3r/RustGPT_Bench_verified\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "if not token:\n",
    "    print(\"Error: Hugging Face access token not provided. Use --token or set the HUGGINGFACE_TOKEN environment variable.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "login(token=token)\n",
    "\n",
    "# Load the dataset\n",
    "print(f\"Loading dataset '{args.dataset_name}' split '{args.split}' from Hugging Face...\")\n",
    "try:\n",
    "    dataset = load_dataset(args.dataset_name, split=args.split)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "dataset =  dataset.remove_columns('r_number')\n",
    "print(\"After removing r_number:\", dataset.column_names)\n",
    "dataset.cast(features)\n",
    "dataset.push_to_hub(args.dataset_name, split=args.split,token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1739f2276344263b8ccbc50842358e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd21d8f34854e1385e391f0c3f9d3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27538e7786aa42ca80a1fb376de73e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d170dcf1099e433db4dd28249ffa1a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ec044caca34c599e535971d7410255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a0e7ad7dea4a9b859a203c51c35216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.cast(features)\n",
    "\n",
    "hyper_dataset = load_dataset(\"r1v3r/hyper_validated\", split=\"train\")\n",
    "hyper_dataset = hyper_dataset.cast(features)\n",
    "# 筛选数据\n",
    "hyper_dataset = hyper_dataset.filter(\n",
    "    lambda x: x[\"instance_id\"] == \"hyperium__hyper-3261\" or x[\"instance_id\"] == \"hyperium__hyper-3275\"\n",
    ")\n",
    "\n",
    "# print(hyper_dataset)\n",
    "serde_dataset = load_dataset(\"r1v3r/serde_validated\", split=\"train\",features=features)\n",
    "serde_dataset = serde_dataset.cast(features)\n",
    "proc_macro2_dataset = load_dataset(\"r1v3r/proc-macro2_validated\", split=\"train\",features=features)\n",
    "proc_macro2_dataset = proc_macro2_dataset.cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b454246ecb8475ca6d72a56698d2812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708d12c9d2a84facb9fd049aa7506c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/r1v3r/RustGPT_Bench_100/commit/78b4725f2f7d1dedcb10cab0d7f0a5ac1e94f89b', commit_message='Upload dataset', commit_description='', oid='78b4725f2f7d1dedcb10cab0d7f0a5ac1e94f89b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/r1v3r/RustGPT_Bench_100', endpoint='https://huggingface.co', repo_type='dataset', repo_id='r1v3r/RustGPT_Bench_100'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "temp = concatenate_datasets([hyper_dataset, serde_dataset, proc_macro2_dataset ,dataset])\n",
    "\n",
    "temp.push_to_hub(\"r1v3r/RustGPT_Bench_100\", token=token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改某个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = None\n",
    "for i, instance in enumerate(dataset):\n",
    "    if instance.get(\"instance_id\") == \"rayon-rs__rayon-986\":\n",
    "        index = i\n",
    "        break\n",
    "\n",
    "if index is not None:\n",
    "    # Update the 'pull_number' for the found instance\n",
    "    dataset = dataset.map(lambda example, idx: \n",
    "                          {**example,\"pull_number\": 986} if idx == index else example, \n",
    "                          with_indices=True)\n",
    "    print(f\"Updated 'pull_number' to 2 for instance with 'instance_id' 1.\")\n",
    "else:\n",
    "    print(\"Error: No instance found with 'instance_id' == 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计buggy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_patch(example):\n",
    "    patch_text = example['patch']\n",
    "    # Use regex to find all lines that indicate file changes\n",
    "    file_changes = re.findall(r'(?m)^(--- |\\+\\+\\+ )([^\\s]+)', patch_text)\n",
    "    # Extract unique file paths\n",
    "    unique_files = set(file for _, file in file_changes)\n",
    "    # Return a dictionary with instance_id and the count of unique files\n",
    "    return {'instance_id': example['instance_id'], 'file_number': len(unique_files)}\n",
    "\n",
    "# Map the function over the dataset\n",
    "results = dataset.map(count_files_in_patch, remove_columns=dataset.column_names)\n",
    "\n",
    "# Convert the results to a dictionary for easy lookup\n",
    "instance_file_map = {result['instance_id']: result['file_number'] for result in results}\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for instance_id, file_number in instance_file_map.items():\n",
    "    print(f\"{instance_id}: {file_number}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问gpt哪些是关于feature的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json  # 用于保存 JSON 文件\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 配置 OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.base_url = \"https://api5.xhub.chat/v1/\"\n",
    "\n",
    "# 模型配置\n",
    "model_name_or_path = \"gpt-4o-mini\"  # 或者 \"gpt-3.5-turbo\"\n",
    "system_messages = \"You are an AI assistant that categorizes tasks into 'Bug Fix' or 'Feature Development'.\"\n",
    "temperature = 0.5\n",
    "top_p = 0.9\n",
    "\n",
    "def classify_problem_statement(problem_statement):\n",
    "    \"\"\"\n",
    "    使用 OpenAI API 对 problem_statement 进行分类。\n",
    "\n",
    "    Args:\n",
    "        problem_statement (str): 问题陈述。\n",
    "\n",
    "    Returns:\n",
    "        str: 分类结果，可能的值为 'Bug Fix', 'Feature Development', 'Unknown', 'Error'。\n",
    "    \"\"\"\n",
    "    user_message = (\n",
    "        f\"Categorize the following problem statement as either 'Bug Fix' or 'Feature Development':\\n\\n\\\"{problem_statement}\\\"\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model_name_or_path,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_messages},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "\n",
    "        # 根据 OpenAI API 的响应格式提取分类结果\n",
    "        classification = response.choices[0].message.content.strip()\n",
    "        print(f\"Classification: {classification}\")\n",
    "\n",
    "        # 确保分类结果为预期的值\n",
    "        if 'Bug Fix' in classification:\n",
    "            return 'Bug Fix'\n",
    "        elif 'Feature Development' in classification:\n",
    "            return 'Feature Development'\n",
    "        else:\n",
    "            return 'Unknown'  # 无法确定的情况\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing problem_statement: {problem_statement}\\nError: {e}\")\n",
    "        return 'Error'\n",
    "\n",
    "\"\"\"\n",
    "主函数，用于加载数据集，分类 problem_statement,并保存结果。\n",
    "\"\"\"\n",
    "# 加载数据集\n",
    "# 初始化一个列表，用于存储分类为 'Feature Development' 的条目\n",
    "feature_development_entries = []\n",
    "\n",
    "# 迭代数据集中的每一项\n",
    "print(\"Classifying problem statements...\")\n",
    "for example in tqdm(dataset, desc=\"Classifying\"):\n",
    "    problem_statement = example.get('problem_statement', None)\n",
    "    instance_id = example.get('instance_id', None)  # 确保 'instance_id' 字段存在\n",
    "\n",
    "    if pd.isna(problem_statement):\n",
    "        classification = 'Unknown'\n",
    "    else:\n",
    "        classification = classify_problem_statement(problem_statement)\n",
    "\n",
    "    # 如果分类为 'Feature Development'，将相关信息添加到列表中\n",
    "    if classification == 'Feature Development':\n",
    "        entry = {\n",
    "            \"instance_id\": instance_id,\n",
    "            \"problem_statement\": problem_statement,\n",
    "            \"response\": classification\n",
    "        }\n",
    "        print(\"instance_id:\", instance_id)\n",
    "        feature_development_entries.append(entry)\n",
    "\n",
    "# 保存分类为 'Feature Development' 的条目到 JSON 文件\n",
    "output_json_path = \"feature_development_entries.json\"  # 指定输出 JSON 文件的路径\n",
    "print(f\"Saving Feature Development entries to {output_json_path}...\")\n",
    "with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(feature_development_entries, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"分类完成，结果已保存。\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = args.dataset_name\n",
    "print(f\"Uploading the cleaned dataset to Hugging Face repository '{repo_id}'...\")\n",
    "\n",
    "dataset.push_to_hub(repo_id, token=token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
