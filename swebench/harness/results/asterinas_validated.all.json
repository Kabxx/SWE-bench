{"repo": "asterinas/asterinas", "pull_number": 1369, "instance_id": "asterinas__asterinas-1369", "issue_numbers": ["919"], "base_commit": "ae4ac384713e63232b74915593ebdef680049d31", "patch": "diff --git a/kernel/src/vm/vmar/mod.rs b/kernel/src/vm/vmar/mod.rs\nindex f691a40453..4cdd850def 100644\n--- a/kernel/src/vm/vmar/mod.rs\n+++ b/kernel/src/vm/vmar/mod.rs\n@@ -16,7 +16,7 @@ use align_ext::AlignExt;\n use aster_rights::Rights;\n use ostd::{\n     cpu::CpuExceptionInfo,\n-    mm::{VmSpace, MAX_USERSPACE_VADDR},\n+    mm::{tlb::TlbFlushOp, PageFlags, PageProperty, VmSpace, MAX_USERSPACE_VADDR},\n };\n \n use self::{\n@@ -220,13 +220,6 @@ impl Vmar_ {\n     }\n \n     fn new_root() -> Arc<Self> {\n-        fn handle_page_fault_wrapper(\n-            vm_space: &VmSpace,\n-            trap_info: &CpuExceptionInfo,\n-        ) -> core::result::Result<(), ()> {\n-            handle_page_fault_from_vm_space(vm_space, &trap_info.try_into().unwrap())\n-        }\n-\n         let mut free_regions = BTreeMap::new();\n         let root_region = FreeRegion::new(ROOT_VMAR_LOWEST_ADDR..ROOT_VMAR_CAP_ADDR);\n         free_regions.insert(root_region.start(), root_region);\n@@ -236,7 +229,7 @@ impl Vmar_ {\n             vm_mappings: BTreeMap::new(),\n             free_regions,\n         };\n-        let vm_space = VmSpace::new();\n+        let mut vm_space = VmSpace::new();\n         vm_space.register_page_fault_handler(handle_page_fault_wrapper);\n         Vmar_::new(vmar_inner, Arc::new(vm_space), 0, ROOT_VMAR_CAP_ADDR, None)\n     }\n@@ -668,17 +661,19 @@ impl Vmar_ {\n             let vm_space = if let Some(parent) = parent {\n                 parent.vm_space().clone()\n             } else {\n-                Arc::new(self.vm_space().fork_copy_on_write())\n+                let mut new_space = VmSpace::new();\n+                new_space.register_page_fault_handler(handle_page_fault_wrapper);\n+                Arc::new(new_space)\n             };\n             Vmar_::new(vmar_inner, vm_space, self.base, self.size, parent)\n         };\n \n         let inner = self.inner.lock();\n+        let mut new_inner = new_vmar_.inner.lock();\n+\n         // Clone free regions.\n         for (free_region_base, free_region) in &inner.free_regions {\n-            new_vmar_\n-                .inner\n-                .lock()\n+            new_inner\n                 .free_regions\n                 .insert(*free_region_base, free_region.clone());\n         }\n@@ -686,26 +681,49 @@ impl Vmar_ {\n         // Clone child vmars.\n         for (child_vmar_base, child_vmar_) in &inner.child_vmar_s {\n             let new_child_vmar = child_vmar_.new_fork(Some(&new_vmar_))?;\n-            new_vmar_\n-                .inner\n-                .lock()\n+            new_inner\n                 .child_vmar_s\n                 .insert(*child_vmar_base, new_child_vmar);\n         }\n \n         // Clone mappings.\n-        for (vm_mapping_base, vm_mapping) in &inner.vm_mappings {\n-            let new_mapping = Arc::new(vm_mapping.new_fork(&new_vmar_)?);\n-            new_vmar_\n-                .inner\n-                .lock()\n-                .vm_mappings\n-                .insert(*vm_mapping_base, new_mapping);\n+        {\n+            let new_vmspace = new_vmar_.vm_space();\n+            let range = self.base..(self.base + self.size);\n+            let mut new_cursor = new_vmspace.cursor_mut(&range).unwrap();\n+            let cur_vmspace = self.vm_space();\n+            let mut cur_cursor = cur_vmspace.cursor_mut(&range).unwrap();\n+            for (vm_mapping_base, vm_mapping) in &inner.vm_mappings {\n+                // Clone the `VmMapping` to the new VMAR.\n+                let new_mapping = Arc::new(vm_mapping.new_fork(&new_vmar_)?);\n+                new_inner.vm_mappings.insert(*vm_mapping_base, new_mapping);\n+\n+                // Protect the mapping and copy to the new page table for COW.\n+                cur_cursor.jump(*vm_mapping_base).unwrap();\n+                new_cursor.jump(*vm_mapping_base).unwrap();\n+                let mut op = |page: &mut PageProperty| {\n+                    page.flags -= PageFlags::W;\n+                };\n+                new_cursor.copy_from(&mut cur_cursor, vm_mapping.map_size(), &mut op);\n+            }\n+            cur_cursor.flusher().issue_tlb_flush(TlbFlushOp::All);\n+            cur_cursor.flusher().dispatch_tlb_flush();\n         }\n+\n+        drop(new_inner);\n+\n         Ok(new_vmar_)\n     }\n }\n \n+/// This is for fallible user space write handling.\n+fn handle_page_fault_wrapper(\n+    vm_space: &VmSpace,\n+    trap_info: &CpuExceptionInfo,\n+) -> core::result::Result<(), ()> {\n+    handle_page_fault_from_vm_space(vm_space, &trap_info.try_into().unwrap())\n+}\n+\n impl<R> Vmar<R> {\n     /// The base address, i.e., the offset relative to the root VMAR.\n     ///\ndiff --git a/kernel/src/vm/vmar/vm_mapping.rs b/kernel/src/vm/vmar/vm_mapping.rs\nindex b5d818e668..87ed53d484 100644\n--- a/kernel/src/vm/vmar/vm_mapping.rs\n+++ b/kernel/src/vm/vmar/vm_mapping.rs\n@@ -11,7 +11,8 @@ use core::{\n use align_ext::AlignExt;\n use aster_rights::Rights;\n use ostd::mm::{\n-    vm_space::VmItem, CachePolicy, Frame, FrameAllocOptions, PageFlags, PageProperty, VmSpace,\n+    tlb::TlbFlushOp, vm_space::VmItem, CachePolicy, Frame, FrameAllocOptions, PageFlags,\n+    PageProperty, VmSpace,\n };\n \n use super::{interval::Interval, is_intersected, Vmar, Vmar_};\n@@ -224,7 +225,7 @@ impl VmMapping {\n \n         match cursor.query().unwrap() {\n             VmItem::Mapped {\n-                va: _,\n+                va,\n                 frame,\n                 mut prop,\n             } if is_write => {\n@@ -245,7 +246,9 @@ impl VmMapping {\n                 let new_flags = PageFlags::W | PageFlags::ACCESSED | PageFlags::DIRTY;\n \n                 if self.is_shared || only_reference {\n-                    cursor.protect(PAGE_SIZE, |p| p.flags |= new_flags);\n+                    cursor.protect_next(PAGE_SIZE, |p| p.flags |= new_flags);\n+                    cursor.flusher().issue_tlb_flush(TlbFlushOp::Address(va));\n+                    cursor.flusher().dispatch_tlb_flush();\n                 } else {\n                     let new_frame = duplicate_frame(&frame)?;\n                     prop.flags |= new_flags;\n@@ -558,7 +561,15 @@ impl VmMappingInner {\n         debug_assert!(range.start % PAGE_SIZE == 0);\n         debug_assert!(range.end % PAGE_SIZE == 0);\n         let mut cursor = vm_space.cursor_mut(&range).unwrap();\n-        cursor.protect(range.len(), |p| p.flags = perms.into());\n+        let op = |p: &mut PageProperty| p.flags = perms.into();\n+        while cursor.virt_addr() < range.end {\n+            if let Some(va) = cursor.protect_next(range.end - cursor.virt_addr(), op) {\n+                cursor.flusher().issue_tlb_flush(TlbFlushOp::Range(va));\n+            } else {\n+                break;\n+            }\n+        }\n+        cursor.flusher().dispatch_tlb_flush();\n         Ok(())\n     }\n \ndiff --git a/ostd/src/mm/mod.rs b/ostd/src/mm/mod.rs\nindex 6bea920ae0..bfe2cafec6 100644\n--- a/ostd/src/mm/mod.rs\n+++ b/ostd/src/mm/mod.rs\n@@ -18,6 +18,7 @@ pub(crate) mod page;\n pub(crate) mod page_prop;\n pub(crate) mod page_table;\n pub mod stat;\n+pub mod tlb;\n pub mod vm_space;\n \n use core::{fmt::Debug, ops::Range};\ndiff --git a/ostd/src/mm/page_table/cursor.rs b/ostd/src/mm/page_table/cursor.rs\nindex a02f490dff..9a643ee1d9 100644\n--- a/ostd/src/mm/page_table/cursor.rs\n+++ b/ostd/src/mm/page_table/cursor.rs\n@@ -734,26 +734,93 @@ where\n         None\n     }\n \n-    pub fn preempt_guard(&self) -> &DisabledPreemptGuard {\n-        &self.0.preempt_guard\n-    }\n-\n-    /// Consumes itself and leak the root guard for the caller if it locked the root level.\n+    /// Copies the mapping from the given cursor to the current cursor.\n     ///\n-    /// It is useful when the caller wants to keep the root guard while the cursor should be dropped.\n-    pub(super) fn leak_root_guard(mut self) -> Option<PageTableNode<E, C>> {\n-        if self.0.guard_level != C::NR_LEVELS {\n-            return None;\n-        }\n+    /// All the mappings in the current cursor's range must be empty. The\n+    /// function allows the source cursor to operate on the mapping before\n+    /// the copy happens. So it is equivalent to protect then duplicate.\n+    /// Only the mapping is copied, the mapped pages are not copied.\n+    ///\n+    /// It can only copy tracked mappings since we consider the untracked\n+    /// mappings not useful to be copied.\n+    ///\n+    /// After the operation, both cursors will advance by the specified length.\n+    ///\n+    /// # Safety\n+    ///\n+    /// The caller should ensure that\n+    ///  - the range being copied with the operation does not affect kernel's\n+    ///    memory safety.\n+    ///  - both of the cursors are in tracked mappings.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This function will panic if:\n+    ///  - either one of the range to be copied is out of the range where any\n+    ///    of the cursor is required to operate;\n+    ///  - either one of the specified virtual address ranges only covers a\n+    ///    part of a page.\n+    ///  - the current cursor's range contains mapped pages.\n+    pub unsafe fn copy_from(\n+        &mut self,\n+        src: &mut Self,\n+        len: usize,\n+        op: &mut impl FnMut(&mut PageProperty),\n+    ) {\n+        assert!(len % page_size::<C>(1) == 0);\n+        let this_end = self.0.va + len;\n+        assert!(this_end <= self.0.barrier_va.end);\n+        let src_end = src.0.va + len;\n+        assert!(src_end <= src.0.barrier_va.end);\n \n-        while self.0.level < C::NR_LEVELS {\n-            self.0.level_up();\n-        }\n+        while self.0.va < this_end && src.0.va < src_end {\n+            let cur_pte = src.0.read_cur_pte();\n+            if !cur_pte.is_present() {\n+                src.0.move_forward();\n+                continue;\n+            }\n+\n+            // Go down if it's not a last node.\n+            if !cur_pte.is_last(src.0.level) {\n+                src.0.level_down();\n+\n+                // We have got down a level. If there's no mapped PTEs in\n+                // the current node, we can go back and skip to save time.\n+                if src.0.guards[(src.0.level - 1) as usize]\n+                    .as_ref()\n+                    .unwrap()\n+                    .nr_children()\n+                    == 0\n+                {\n+                    src.0.level_up();\n+                    src.0.move_forward();\n+                }\n+\n+                continue;\n+            }\n \n-        self.0.guards[(C::NR_LEVELS - 1) as usize].take()\n+            // Do protection.\n+            let mut pte_prop = cur_pte.prop();\n+            op(&mut pte_prop);\n+\n+            let idx = src.0.cur_idx();\n+            src.cur_node_mut().protect(idx, pte_prop);\n \n-        // Ok to drop the cursor here because we ensure not to access the page table if the current\n-        // level is the root level when running the dropping method.\n+            // Do copy.\n+            let child = src.cur_node_mut().child(idx, true);\n+            let Child::<E, C>::Page(page, prop) = child else {\n+                panic!(\"Unexpected child for source mapping: {:#?}\", child);\n+            };\n+            self.jump(src.0.va).unwrap();\n+            let mapped_page_size = page.size();\n+            let original = self.map(page, prop);\n+            debug_assert!(original.is_none());\n+\n+            // Only move the source cursor forward since `Self::map` will do it.\n+            // This assertion is to ensure that they move by the same length.\n+            debug_assert_eq!(mapped_page_size, page_size::<C>(src.0.level));\n+            src.0.move_forward();\n+        }\n     }\n \n     /// Goes down a level assuming the current slot is absent.\ndiff --git a/ostd/src/mm/page_table/mod.rs b/ostd/src/mm/page_table/mod.rs\nindex 9bb1e2cc62..a8294c38da 100644\n--- a/ostd/src/mm/page_table/mod.rs\n+++ b/ostd/src/mm/page_table/mod.rs\n@@ -92,53 +92,29 @@ impl PageTable<UserMode> {\n             self.root.activate();\n         }\n     }\n-\n-    /// Create a cloned new page table.\n-    ///\n-    /// This method takes a mutable cursor to the old page table that locks the\n-    /// entire virtual address range. The caller may implement the copy-on-write\n-    /// mechanism by first protecting the old page table and then clone it using\n-    /// this method.\n-    ///\n-    /// TODO: We may consider making the page table itself copy-on-write.\n-    pub fn clone_with(\n-        &self,\n-        cursor: CursorMut<'_, UserMode, PageTableEntry, PagingConsts>,\n-    ) -> Self {\n-        let root_node = cursor.leak_root_guard().unwrap();\n-\n-        const NR_PTES_PER_NODE: usize = nr_subpage_per_huge::<PagingConsts>();\n-        let new_root_node = unsafe {\n-            root_node.make_copy(\n-                0..NR_PTES_PER_NODE / 2,\n-                NR_PTES_PER_NODE / 2..NR_PTES_PER_NODE,\n-            )\n-        };\n-\n-        PageTable::<UserMode> {\n-            root: new_root_node.into_raw(),\n-            _phantom: PhantomData,\n-        }\n-    }\n }\n \n impl PageTable<KernelMode> {\n     /// Create a new user page table.\n     ///\n-    /// This should be the only way to create the first user page table, that is\n-    /// to fork the kernel page table with all the kernel mappings shared.\n-    ///\n-    /// Then, one can use a user page table to call [`fork_copy_on_write`], creating\n-    /// other child page tables.\n+    /// This should be the only way to create the user page table, that is to\n+    /// duplicate the kernel page table with all the kernel mappings shared.\n     pub fn create_user_page_table(&self) -> PageTable<UserMode> {\n         let root_node = self.root.clone_shallow().lock();\n+        let mut new_node = PageTableNode::alloc(PagingConsts::NR_LEVELS);\n \n+        // Make a shallow copy of the root node in the kernel space range.\n+        // The user space range is not copied.\n         const NR_PTES_PER_NODE: usize = nr_subpage_per_huge::<PagingConsts>();\n-        let new_root_node =\n-            unsafe { root_node.make_copy(0..0, NR_PTES_PER_NODE / 2..NR_PTES_PER_NODE) };\n+        for i in NR_PTES_PER_NODE / 2..NR_PTES_PER_NODE {\n+            let child = root_node.child(i, /* meaningless */ true);\n+            if !child.is_none() {\n+                let _ = new_node.replace_child(i, child, /* meaningless */ true);\n+            }\n+        }\n \n         PageTable::<UserMode> {\n-            root: new_root_node.into_raw(),\n+            root: new_node.into_raw(),\n             _phantom: PhantomData,\n         }\n     }\ndiff --git a/ostd/src/mm/page_table/node.rs b/ostd/src/mm/page_table/node.rs\nindex 134cd112a0..f39d9daf57 100644\n--- a/ostd/src/mm/page_table/node.rs\n+++ b/ostd/src/mm/page_table/node.rs\n@@ -25,9 +25,7 @@\n //! the initialization of the entity that the PTE points to. This is taken care in this module.\n //!\n \n-use core::{\n-    fmt, marker::PhantomData, mem::ManuallyDrop, ops::Range, panic, sync::atomic::Ordering,\n-};\n+use core::{fmt, marker::PhantomData, mem::ManuallyDrop, panic, sync::atomic::Ordering};\n \n use super::{nr_subpage_per_huge, page_size, PageTableEntryTrait};\n use crate::{\n@@ -374,74 +372,6 @@ where\n         }\n     }\n \n-    /// Makes a copy of the page table node.\n-    ///\n-    /// This function allows you to control about the way to copy the children.\n-    /// For indexes in `deep`, the children are deep copied and this function will be recursively called.\n-    /// For indexes in `shallow`, the children are shallow copied as new references.\n-    ///\n-    /// You cannot shallow copy a child that is mapped to a page. Deep copying a page child will not\n-    /// copy the mapped page but will copy the handle to the page.\n-    ///\n-    /// You cannot either deep copy or shallow copy a child that is mapped to an untracked page.\n-    ///\n-    /// The ranges must be disjoint.\n-    pub(super) unsafe fn make_copy(&self, deep: Range<usize>, shallow: Range<usize>) -> Self {\n-        debug_assert!(deep.end <= nr_subpage_per_huge::<C>());\n-        debug_assert!(shallow.end <= nr_subpage_per_huge::<C>());\n-        debug_assert!(deep.end <= shallow.start || deep.start >= shallow.end);\n-\n-        let mut new_pt = Self::alloc(self.level());\n-        let mut copied_child_count = self.nr_children();\n-        for i in deep {\n-            if copied_child_count == 0 {\n-                return new_pt;\n-            }\n-            match self.child(i, true) {\n-                Child::PageTable(pt) => {\n-                    let guard = pt.clone_shallow().lock();\n-                    let new_child = guard.make_copy(0..nr_subpage_per_huge::<C>(), 0..0);\n-                    let old = new_pt.replace_child(i, Child::PageTable(new_child.into_raw()), true);\n-                    debug_assert!(old.is_none());\n-                    copied_child_count -= 1;\n-                }\n-                Child::Page(page, prop) => {\n-                    let old = new_pt.replace_child(i, Child::Page(page.clone(), prop), true);\n-                    debug_assert!(old.is_none());\n-                    copied_child_count -= 1;\n-                }\n-                Child::None => {}\n-                Child::Untracked(_, _) => {\n-                    unreachable!();\n-                }\n-            }\n-        }\n-\n-        for i in shallow {\n-            if copied_child_count == 0 {\n-                return new_pt;\n-            }\n-            debug_assert_eq!(self.level(), C::NR_LEVELS);\n-            match self.child(i, /*meaningless*/ true) {\n-                Child::PageTable(pt) => {\n-                    let old = new_pt.replace_child(\n-                        i,\n-                        Child::PageTable(pt.clone_shallow()),\n-                        /*meaningless*/ true,\n-                    );\n-                    debug_assert!(old.is_none());\n-                    copied_child_count -= 1;\n-                }\n-                Child::None => {}\n-                Child::Page(_, _) | Child::Untracked(_, _) => {\n-                    unreachable!();\n-                }\n-            }\n-        }\n-\n-        new_pt\n-    }\n-\n     /// Splits the untracked huge page mapped at `idx` to smaller pages.\n     pub(super) fn split_untracked_huge(&mut self, idx: usize) {\n         // These should be ensured by the cursor.\ndiff --git a/ostd/src/mm/tlb.rs b/ostd/src/mm/tlb.rs\nnew file mode 100644\nindex 0000000000..4ec58e091e\n--- /dev/null\n+++ b/ostd/src/mm/tlb.rs\n@@ -0,0 +1,222 @@\n+// SPDX-License-Identifier: MPL-2.0\n+\n+//! TLB flush operations.\n+\n+use alloc::vec::Vec;\n+use core::ops::Range;\n+\n+use super::{page::DynPage, Vaddr, PAGE_SIZE};\n+use crate::{\n+    cpu::{CpuSet, PinCurrentCpu},\n+    cpu_local,\n+    sync::SpinLock,\n+    task::disable_preempt,\n+};\n+\n+/// A TLB flusher that is aware of which CPUs are needed to be flushed.\n+///\n+/// The flusher needs to stick to the current CPU.\n+pub struct TlbFlusher<G: PinCurrentCpu> {\n+    target_cpus: CpuSet,\n+    // Better to store them here since loading and counting them from the CPUs\n+    // list brings non-trivial overhead.\n+    need_remote_flush: bool,\n+    need_self_flush: bool,\n+    _pin_current: G,\n+}\n+\n+impl<G: PinCurrentCpu> TlbFlusher<G> {\n+    /// Creates a new TLB flusher with the specified CPUs to be flushed.\n+    ///\n+    /// The flusher needs to stick to the current CPU. So please provide a\n+    /// guard that implements [`PinCurrentCpu`].\n+    pub fn new(target_cpus: CpuSet, pin_current_guard: G) -> Self {\n+        let current_cpu = pin_current_guard.current_cpu();\n+\n+        let mut need_self_flush = false;\n+        let mut need_remote_flush = false;\n+\n+        for cpu in target_cpus.iter() {\n+            if cpu == current_cpu {\n+                need_self_flush = true;\n+            } else {\n+                need_remote_flush = true;\n+            }\n+        }\n+        Self {\n+            target_cpus,\n+            need_remote_flush,\n+            need_self_flush,\n+            _pin_current: pin_current_guard,\n+        }\n+    }\n+\n+    /// Issues a pending TLB flush request.\n+    ///\n+    /// On SMP systems, the notification is sent to all the relevant CPUs only\n+    /// when [`Self::dispatch_tlb_flush`] is called.\n+    pub fn issue_tlb_flush(&self, op: TlbFlushOp) {\n+        self.issue_tlb_flush_(op, None);\n+    }\n+\n+    /// Dispatches all the pending TLB flush requests.\n+    ///\n+    /// The pending requests are issued by [`Self::issue_tlb_flush`].\n+    pub fn dispatch_tlb_flush(&self) {\n+        if !self.need_remote_flush {\n+            return;\n+        }\n+\n+        crate::smp::inter_processor_call(&self.target_cpus, do_remote_flush);\n+    }\n+\n+    /// Issues a TLB flush request that must happen before dropping the page.\n+    ///\n+    /// If we need to remove a mapped page from the page table, we can only\n+    /// recycle the page after all the relevant TLB entries in all CPUs are\n+    /// flushed. Otherwise if the page is recycled for other purposes, the user\n+    /// space program can still access the page through the TLB entries. This\n+    /// method is designed to be used in such cases.\n+    pub fn issue_tlb_flush_with(&self, op: TlbFlushOp, drop_after_flush: DynPage) {\n+        self.issue_tlb_flush_(op, Some(drop_after_flush));\n+    }\n+\n+    /// Whether the TLB flusher needs to flush the TLB entries on other CPUs.\n+    pub fn need_remote_flush(&self) -> bool {\n+        self.need_remote_flush\n+    }\n+\n+    /// Whether the TLB flusher needs to flush the TLB entries on the current CPU.\n+    pub fn need_self_flush(&self) -> bool {\n+        self.need_self_flush\n+    }\n+\n+    fn issue_tlb_flush_(&self, op: TlbFlushOp, drop_after_flush: Option<DynPage>) {\n+        let op = op.optimize_for_large_range();\n+\n+        // Fast path for single CPU cases.\n+        if !self.need_remote_flush {\n+            if self.need_self_flush {\n+                op.perform_on_current();\n+            }\n+            return;\n+        }\n+\n+        // Slow path for multi-CPU cases.\n+        for cpu in self.target_cpus.iter() {\n+            let mut op_queue = FLUSH_OPS.get_on_cpu(cpu).lock();\n+            if let Some(drop_after_flush) = drop_after_flush.clone() {\n+                PAGE_KEEPER.get_on_cpu(cpu).lock().push(drop_after_flush);\n+            }\n+            op_queue.push(op.clone());\n+        }\n+    }\n+}\n+\n+/// The operation to flush TLB entries.\n+#[derive(Debug, Clone)]\n+pub enum TlbFlushOp {\n+    /// Flush all TLB entries except for the global entries.\n+    All,\n+    /// Flush the TLB entry for the specified virtual address.\n+    Address(Vaddr),\n+    /// Flush the TLB entries for the specified virtual address range.\n+    Range(Range<Vaddr>),\n+}\n+\n+impl TlbFlushOp {\n+    /// Performs the TLB flush operation on the current CPU.\n+    pub fn perform_on_current(&self) {\n+        use crate::arch::mm::{\n+            tlb_flush_addr, tlb_flush_addr_range, tlb_flush_all_excluding_global,\n+        };\n+        match self {\n+            TlbFlushOp::All => tlb_flush_all_excluding_global(),\n+            TlbFlushOp::Address(addr) => tlb_flush_addr(*addr),\n+            TlbFlushOp::Range(range) => tlb_flush_addr_range(range),\n+        }\n+    }\n+\n+    fn optimize_for_large_range(self) -> Self {\n+        match self {\n+            TlbFlushOp::Range(range) => {\n+                if range.len() > FLUSH_ALL_RANGE_THRESHOLD {\n+                    TlbFlushOp::All\n+                } else {\n+                    TlbFlushOp::Range(range)\n+                }\n+            }\n+            _ => self,\n+        }\n+    }\n+}\n+\n+// The queues of pending requests on each CPU.\n+//\n+// Lock ordering: lock FLUSH_OPS before PAGE_KEEPER.\n+cpu_local! {\n+    static FLUSH_OPS: SpinLock<OpsStack> = SpinLock::new(OpsStack::new());\n+    static PAGE_KEEPER: SpinLock<Vec<DynPage>> = SpinLock::new(Vec::new());\n+}\n+\n+fn do_remote_flush() {\n+    let preempt_guard = disable_preempt();\n+    let current_cpu = preempt_guard.current_cpu();\n+\n+    let mut op_queue = FLUSH_OPS.get_on_cpu(current_cpu).lock();\n+    op_queue.flush_all();\n+    PAGE_KEEPER.get_on_cpu(current_cpu).lock().clear();\n+}\n+\n+/// If a TLB flushing request exceeds this threshold, we flush all.\n+pub(crate) const FLUSH_ALL_RANGE_THRESHOLD: usize = 32 * PAGE_SIZE;\n+\n+/// If the number of pending requests exceeds this threshold, we flush all the\n+/// TLB entries instead of flushing them one by one.\n+const FLUSH_ALL_OPS_THRESHOLD: usize = 32;\n+\n+struct OpsStack {\n+    ops: [Option<TlbFlushOp>; FLUSH_ALL_OPS_THRESHOLD],\n+    need_flush_all: bool,\n+    size: usize,\n+}\n+\n+impl OpsStack {\n+    const fn new() -> Self {\n+        const ARRAY_REPEAT_VALUE: Option<TlbFlushOp> = None;\n+        Self {\n+            ops: [ARRAY_REPEAT_VALUE; FLUSH_ALL_OPS_THRESHOLD],\n+            need_flush_all: false,\n+            size: 0,\n+        }\n+    }\n+\n+    fn push(&mut self, op: TlbFlushOp) {\n+        if self.need_flush_all {\n+            return;\n+        }\n+\n+        if self.size < FLUSH_ALL_OPS_THRESHOLD {\n+            self.ops[self.size] = Some(op);\n+            self.size += 1;\n+        } else {\n+            self.need_flush_all = true;\n+            self.size = 0;\n+        }\n+    }\n+\n+    fn flush_all(&mut self) {\n+        if self.need_flush_all {\n+            crate::arch::mm::tlb_flush_all_excluding_global();\n+            self.need_flush_all = false;\n+        } else {\n+            for i in 0..self.size {\n+                if let Some(op) = &self.ops[i] {\n+                    op.perform_on_current();\n+                }\n+            }\n+        }\n+\n+        self.size = 0;\n+    }\n+}\ndiff --git a/ostd/src/mm/vm_space.rs b/ostd/src/mm/vm_space.rs\nindex 78109d9b8e..3c55d6c3bd 100644\n--- a/ostd/src/mm/vm_space.rs\n+++ b/ostd/src/mm/vm_space.rs\n@@ -9,32 +9,25 @@\n //! powerful concurrent accesses to the page table, and suffers from the same\n //! validity concerns as described in [`super::page_table::cursor`].\n \n-use alloc::collections::vec_deque::VecDeque;\n use core::{\n     ops::Range,\n     sync::atomic::{AtomicPtr, Ordering},\n };\n \n-use spin::Once;\n-\n-use super::{\n-    io::Fallible,\n-    kspace::KERNEL_PAGE_TABLE,\n-    page::DynPage,\n-    page_table::{PageTable, UserMode},\n-    PageFlags, PageProperty, VmReader, VmWriter, PAGE_SIZE,\n-};\n use crate::{\n     arch::mm::{current_page_table_paddr, PageTableEntry, PagingConsts},\n     cpu::{num_cpus, CpuExceptionInfo, CpuSet, PinCurrentCpu},\n     cpu_local,\n     mm::{\n-        page_table::{self, PageTableItem},\n-        Frame, MAX_USERSPACE_VADDR,\n+        io::Fallible,\n+        kspace::KERNEL_PAGE_TABLE,\n+        page_table::{self, PageTable, PageTableItem, UserMode},\n+        tlb::{TlbFlushOp, TlbFlusher, FLUSH_ALL_RANGE_THRESHOLD},\n+        Frame, PageProperty, VmReader, VmWriter, MAX_USERSPACE_VADDR,\n     },\n     prelude::*,\n-    sync::{RwLock, RwLockReadGuard, SpinLock},\n-    task::disable_preempt,\n+    sync::{RwLock, RwLockReadGuard},\n+    task::{disable_preempt, DisabledPreemptGuard},\n     Error,\n };\n \n@@ -56,7 +49,7 @@ use crate::{\n #[derive(Debug)]\n pub struct VmSpace {\n     pt: PageTable<UserMode>,\n-    page_fault_handler: Once<fn(&VmSpace, &CpuExceptionInfo) -> core::result::Result<(), ()>>,\n+    page_fault_handler: Option<fn(&VmSpace, &CpuExceptionInfo) -> core::result::Result<(), ()>>,\n     /// A CPU can only activate a `VmSpace` when no mutable cursors are alive.\n     /// Cursors hold read locks and activation require a write lock.\n     activation_lock: RwLock<()>,\n@@ -67,7 +60,7 @@ impl VmSpace {\n     pub fn new() -> Self {\n         Self {\n             pt: KERNEL_PAGE_TABLE.get().unwrap().create_user_page_table(),\n-            page_fault_handler: Once::new(),\n+            page_fault_handler: None,\n             activation_lock: RwLock::new(()),\n         }\n     }\n@@ -98,11 +91,7 @@ impl VmSpace {\n         Ok(self.pt.cursor_mut(va).map(|pt_cursor| {\n             let activation_lock = self.activation_lock.read();\n \n-            let cur_cpu = pt_cursor.preempt_guard().current_cpu();\n-\n             let mut activated_cpus = CpuSet::new_empty();\n-            let mut need_self_flush = false;\n-            let mut need_remote_flush = false;\n \n             for cpu in 0..num_cpus() {\n                 // The activation lock is held; other CPUs cannot activate this `VmSpace`.\n@@ -110,20 +99,13 @@ impl VmSpace {\n                     ACTIVATED_VM_SPACE.get_on_cpu(cpu).load(Ordering::Relaxed) as *const VmSpace;\n                 if ptr == self as *const VmSpace {\n                     activated_cpus.add(cpu);\n-                    if cpu == cur_cpu {\n-                        need_self_flush = true;\n-                    } else {\n-                        need_remote_flush = true;\n-                    }\n                 }\n             }\n \n             CursorMut {\n                 pt_cursor,\n                 activation_lock,\n-                activated_cpus,\n-                need_remote_flush,\n-                need_self_flush,\n+                flusher: TlbFlusher::new(activated_cpus, disable_preempt()),\n             }\n         })?)\n     }\n@@ -156,63 +138,18 @@ impl VmSpace {\n         &self,\n         info: &CpuExceptionInfo,\n     ) -> core::result::Result<(), ()> {\n-        if let Some(func) = self.page_fault_handler.get() {\n+        if let Some(func) = self.page_fault_handler {\n             return func(self, info);\n         }\n         Err(())\n     }\n \n     /// Registers the page fault handler in this `VmSpace`.\n-    ///\n-    /// The page fault handler of a `VmSpace` can only be initialized once.\n-    /// If it has been initialized before, calling this method will have no effect.\n     pub fn register_page_fault_handler(\n-        &self,\n+        &mut self,\n         func: fn(&VmSpace, &CpuExceptionInfo) -> core::result::Result<(), ()>,\n     ) {\n-        self.page_fault_handler.call_once(|| func);\n-    }\n-\n-    /// Forks a new VM space with copy-on-write semantics.\n-    ///\n-    /// Both the parent and the newly forked VM space will be marked as\n-    /// read-only. And both the VM space will take handles to the same\n-    /// physical memory pages.\n-    pub fn fork_copy_on_write(&self) -> Self {\n-        // Protect the parent VM space as read-only.\n-        let end = MAX_USERSPACE_VADDR;\n-        let mut cursor = self.cursor_mut(&(0..end)).unwrap();\n-        let mut op = |prop: &mut PageProperty| {\n-            prop.flags -= PageFlags::W;\n-        };\n-\n-        cursor.protect(end, &mut op);\n-\n-        let page_fault_handler = {\n-            let new_handler = Once::new();\n-            if let Some(handler) = self.page_fault_handler.get() {\n-                new_handler.call_once(|| *handler);\n-            }\n-            new_handler\n-        };\n-\n-        let CursorMut {\n-            pt_cursor,\n-            activation_lock,\n-            ..\n-        } = cursor;\n-\n-        let new_pt = self.pt.clone_with(pt_cursor);\n-\n-        // Release the activation lock after the page table is cloned to\n-        // prevent modification to the parent page table while cloning.\n-        drop(activation_lock);\n-\n-        Self {\n-            pt: new_pt,\n-            page_fault_handler,\n-            activation_lock: RwLock::new(()),\n-        }\n+        self.page_fault_handler = Some(func);\n     }\n \n     /// Creates a reader to read data from the user space of the current task.\n@@ -311,12 +248,9 @@ pub struct CursorMut<'a, 'b> {\n     pt_cursor: page_table::CursorMut<'a, UserMode, PageTableEntry, PagingConsts>,\n     #[allow(dead_code)]\n     activation_lock: RwLockReadGuard<'b, ()>,\n-    // Better to store them here since loading and counting them from the CPUs\n-    // list brings non-trivial overhead. We have a read lock so the stored set\n-    // is always a superset of actual activated CPUs.\n-    activated_cpus: CpuSet,\n-    need_remote_flush: bool,\n-    need_self_flush: bool,\n+    // We have a read lock so the CPU set in the flusher is always a superset\n+    // of actual activated CPUs.\n+    flusher: TlbFlusher<DisabledPreemptGuard>,\n }\n \n impl CursorMut<'_, '_> {\n@@ -345,6 +279,11 @@ impl CursorMut<'_, '_> {\n         self.pt_cursor.virt_addr()\n     }\n \n+    /// Get the dedicated TLB flusher for this cursor.\n+    pub fn flusher(&self) -> &TlbFlusher<DisabledPreemptGuard> {\n+        &self.flusher\n+    }\n+\n     /// Map a frame into the current slot.\n     ///\n     /// This method will bring the cursor to the next slot after the modification.\n@@ -353,9 +292,10 @@ impl CursorMut<'_, '_> {\n         // SAFETY: It is safe to map untyped memory into the userspace.\n         let old = unsafe { self.pt_cursor.map(frame.into(), prop) };\n \n-        if old.is_some() {\n-            self.issue_tlb_flush(TlbFlushOp::Address(start_va), old);\n-            self.dispatch_tlb_flush();\n+        if let Some(old) = old {\n+            self.flusher\n+                .issue_tlb_flush_with(TlbFlushOp::Address(start_va), old);\n+            self.flusher.dispatch_tlb_flush();\n         }\n     }\n \n@@ -367,25 +307,31 @@ impl CursorMut<'_, '_> {\n     /// Already-absent mappings encountered by the cursor will be skipped. It\n     /// is valid to unmap a range that is not mapped.\n     ///\n+    /// It must issue and dispatch a TLB flush after the operation. Otherwise,\n+    /// the memory safety will be compromised. Please call this function less\n+    /// to avoid the overhead of TLB flush. Using a large `len` is wiser than\n+    /// splitting the operation into multiple small ones.\n+    ///\n     /// # Panics\n     ///\n     /// This method will panic if `len` is not page-aligned.\n     pub fn unmap(&mut self, len: usize) {\n         assert!(len % super::PAGE_SIZE == 0);\n         let end_va = self.virt_addr() + len;\n-        let tlb_prefer_flush_all = len > TLB_FLUSH_ALL_THRESHOLD * PAGE_SIZE;\n+        let tlb_prefer_flush_all = len > FLUSH_ALL_RANGE_THRESHOLD;\n \n         loop {\n             // SAFETY: It is safe to un-map memory in the userspace.\n             let result = unsafe { self.pt_cursor.take_next(end_va - self.virt_addr()) };\n             match result {\n                 PageTableItem::Mapped { va, page, .. } => {\n-                    if !self.need_remote_flush && tlb_prefer_flush_all {\n+                    if !self.flusher.need_remote_flush() && tlb_prefer_flush_all {\n                         // Only on single-CPU cases we can drop the page immediately before flushing.\n                         drop(page);\n                         continue;\n                     }\n-                    self.issue_tlb_flush(TlbFlushOp::Address(va), Some(page));\n+                    self.flusher\n+                        .issue_tlb_flush_with(TlbFlushOp::Address(va), page);\n                 }\n                 PageTableItem::NotMapped { .. } => {\n                     break;\n@@ -396,103 +342,79 @@ impl CursorMut<'_, '_> {\n             }\n         }\n \n-        if !self.need_remote_flush && tlb_prefer_flush_all {\n-            self.issue_tlb_flush(TlbFlushOp::All, None);\n+        if !self.flusher.need_remote_flush() && tlb_prefer_flush_all {\n+            self.flusher.issue_tlb_flush(TlbFlushOp::All);\n         }\n \n-        self.dispatch_tlb_flush();\n+        self.flusher.dispatch_tlb_flush();\n     }\n \n-    /// Change the mapping property starting from the current slot.\n+    /// Applies the operation to the next slot of mapping within the range.\n     ///\n-    /// This method will bring the cursor forward by `len` bytes in the virtual\n-    /// address space after the modification.\n+    /// The range to be found in is the current virtual address with the\n+    /// provided length.\n+    ///\n+    /// The function stops and yields the actually protected range if it has\n+    /// actually protected a page, no matter if the following pages are also\n+    /// required to be protected.\n+    ///\n+    /// It also makes the cursor moves forward to the next page after the\n+    /// protected one. If no mapped pages exist in the following range, the\n+    /// cursor will stop at the end of the range and return [`None`].\n     ///\n-    /// The way to change the property is specified by the closure `op`.\n+    /// Note that it will **NOT** flush the TLB after the operation. Please\n+    /// make the decision yourself on when and how to flush the TLB using\n+    /// [`Self::flusher`].\n     ///\n     /// # Panics\n     ///\n-    /// This method will panic if `len` is not page-aligned.\n-    pub fn protect(&mut self, len: usize, mut op: impl FnMut(&mut PageProperty)) {\n-        assert!(len % super::PAGE_SIZE == 0);\n-        let end = self.virt_addr() + len;\n-        let tlb_prefer_flush_all = len > TLB_FLUSH_ALL_THRESHOLD * PAGE_SIZE;\n-\n+    /// This function will panic if:\n+    ///  - the range to be protected is out of the range where the cursor\n+    ///    is required to operate;\n+    ///  - the specified virtual address range only covers a part of a page.\n+    pub fn protect_next(\n+        &mut self,\n+        len: usize,\n+        mut op: impl FnMut(&mut PageProperty),\n+    ) -> Option<Range<Vaddr>> {\n         // SAFETY: It is safe to protect memory in the userspace.\n-        while let Some(range) =\n-            unsafe { self.pt_cursor.protect_next(end - self.virt_addr(), &mut op) }\n-        {\n-            if !tlb_prefer_flush_all {\n-                self.issue_tlb_flush(TlbFlushOp::Range(range), None);\n-            }\n-        }\n-\n-        if tlb_prefer_flush_all {\n-            self.issue_tlb_flush(TlbFlushOp::All, None);\n-        }\n-        self.dispatch_tlb_flush();\n+        unsafe { self.pt_cursor.protect_next(len, &mut op) }\n     }\n \n-    fn issue_tlb_flush(&self, op: TlbFlushOp, drop_after_flush: Option<DynPage>) {\n-        let request = TlbFlushRequest {\n-            op,\n-            drop_after_flush,\n-        };\n-\n-        // Fast path for single CPU cases.\n-        if !self.need_remote_flush {\n-            if self.need_self_flush {\n-                request.do_flush();\n-            }\n-            return;\n-        }\n-\n-        // Slow path for multi-CPU cases.\n-        for cpu in self.activated_cpus.iter() {\n-            let mut queue = TLB_FLUSH_REQUESTS.get_on_cpu(cpu).lock();\n-            queue.push_back(request.clone());\n-        }\n-    }\n-\n-    fn dispatch_tlb_flush(&self) {\n-        if !self.need_remote_flush {\n-            return;\n-        }\n-\n-        fn do_remote_flush() {\n-            let preempt_guard = disable_preempt();\n-            let mut requests = TLB_FLUSH_REQUESTS\n-                .get_on_cpu(preempt_guard.current_cpu())\n-                .lock();\n-            if requests.len() > TLB_FLUSH_ALL_THRESHOLD {\n-                // TODO: in most cases, we need only to flush all the TLB entries\n-                // for an ASID if it is enabled.\n-                crate::arch::mm::tlb_flush_all_excluding_global();\n-                requests.clear();\n-            } else {\n-                while let Some(request) = requests.pop_front() {\n-                    request.do_flush();\n-                    if matches!(request.op, TlbFlushOp::All) {\n-                        requests.clear();\n-                        break;\n-                    }\n-                }\n-            }\n-        }\n-\n-        crate::smp::inter_processor_call(&self.activated_cpus.clone(), do_remote_flush);\n+    /// Copies the mapping from the given cursor to the current cursor.\n+    ///\n+    /// All the mappings in the current cursor's range must be empty. The\n+    /// function allows the source cursor to operate on the mapping before\n+    /// the copy happens. So it is equivalent to protect then duplicate.\n+    /// Only the mapping is copied, the mapped pages are not copied.\n+    ///\n+    /// After the operation, both cursors will advance by the specified length.\n+    ///\n+    /// Note that it will **NOT** flush the TLB after the operation. Please\n+    /// make the decision yourself on when and how to flush the TLB using\n+    /// the source's [`CursorMut::flusher`].\n+    ///\n+    /// # Panics\n+    ///\n+    /// This function will panic if:\n+    ///  - either one of the range to be copied is out of the range where any\n+    ///    of the cursor is required to operate;\n+    ///  - either one of the specified virtual address ranges only covers a\n+    ///    part of a page.\n+    ///  - the current cursor's range contains mapped pages.\n+    pub fn copy_from(\n+        &mut self,\n+        src: &mut Self,\n+        len: usize,\n+        op: &mut impl FnMut(&mut PageProperty),\n+    ) {\n+        // SAFETY: Operations on user memory spaces are safe if it doesn't\n+        // involve dropping any pages.\n+        unsafe { self.pt_cursor.copy_from(&mut src.pt_cursor, len, op) }\n     }\n }\n \n-/// The threshold used to determine whether we need to flush all TLB entries\n-/// when handling a bunch of TLB flush requests. If the number of requests\n-/// exceeds this threshold, the overhead incurred by flushing pages\n-/// individually would surpass the overhead of flushing all entries at once.\n-const TLB_FLUSH_ALL_THRESHOLD: usize = 32;\n-\n cpu_local! {\n-    /// The queue of pending requests.\n-    static TLB_FLUSH_REQUESTS: SpinLock<VecDeque<TlbFlushRequest>> = SpinLock::new(VecDeque::new());\n     /// The `Arc` pointer to the activated VM space on this CPU. If the pointer\n     /// is NULL, it means that the activated page table is merely the kernel\n     /// page table.\n@@ -502,38 +424,6 @@ cpu_local! {\n     static ACTIVATED_VM_SPACE: AtomicPtr<VmSpace> = AtomicPtr::new(core::ptr::null_mut());\n }\n \n-#[derive(Debug, Clone)]\n-struct TlbFlushRequest {\n-    op: TlbFlushOp,\n-    // If we need to remove a mapped page from the page table, we can only\n-    // recycle the page after all the relevant TLB entries in all CPUs are\n-    // flushed. Otherwise if the page is recycled for other purposes, the user\n-    // space program can still access the page through the TLB entries.\n-    #[allow(dead_code)]\n-    drop_after_flush: Option<DynPage>,\n-}\n-\n-#[derive(Debug, Clone)]\n-enum TlbFlushOp {\n-    All,\n-    Address(Vaddr),\n-    Range(Range<Vaddr>),\n-}\n-\n-impl TlbFlushRequest {\n-    /// Perform the TLB flush operation on the current CPU.\n-    fn do_flush(&self) {\n-        use crate::arch::mm::{\n-            tlb_flush_addr, tlb_flush_addr_range, tlb_flush_all_excluding_global,\n-        };\n-        match &self.op {\n-            TlbFlushOp::All => tlb_flush_all_excluding_global(),\n-            TlbFlushOp::Address(addr) => tlb_flush_addr(*addr),\n-            TlbFlushOp::Range(range) => tlb_flush_addr_range(range),\n-        }\n-    }\n-}\n-\n /// The result of a query over the VM space.\n #[derive(Debug)]\n pub enum VmItem {\n", "test_patch": "diff --git a/ostd/src/mm/page_table/test.rs b/ostd/src/mm/page_table/test.rs\nindex 834289a910..6acb5bc22b 100644\n--- a/ostd/src/mm/page_table/test.rs\n+++ b/ostd/src/mm/page_table/test.rs\n@@ -81,6 +81,10 @@ fn test_untracked_map_unmap() {\n \n #[ktest]\n fn test_user_copy_on_write() {\n+    fn prot_op(prop: &mut PageProperty) {\n+        prop.flags -= PageFlags::W;\n+    }\n+\n     let pt = PageTable::<UserMode>::empty();\n     let from = PAGE_SIZE..PAGE_SIZE * 2;\n     let page = allocator::alloc_single(FrameMeta::default()).unwrap();\n@@ -96,7 +100,14 @@ fn test_user_copy_on_write() {\n     unsafe { pt.cursor_mut(&from).unwrap().map(page.clone().into(), prop) };\n     assert_eq!(pt.query(from.start + 10).unwrap().0, start_paddr + 10);\n \n-    let child_pt = pt.clone_with(pt.cursor_mut(&(0..MAX_USERSPACE_VADDR)).unwrap());\n+    let child_pt = {\n+        let child_pt = PageTable::<UserMode>::empty();\n+        let range = 0..MAX_USERSPACE_VADDR;\n+        let mut child_cursor = child_pt.cursor_mut(&range).unwrap();\n+        let mut parent_cursor = pt.cursor_mut(&range).unwrap();\n+        unsafe { child_cursor.copy_from(&mut parent_cursor, range.len(), &mut prot_op) };\n+        child_pt\n+    };\n     assert_eq!(pt.query(from.start + 10).unwrap().0, start_paddr + 10);\n     assert_eq!(child_pt.query(from.start + 10).unwrap().0, start_paddr + 10);\n     assert!(matches!(\n@@ -106,7 +117,14 @@ fn test_user_copy_on_write() {\n     assert!(pt.query(from.start + 10).is_none());\n     assert_eq!(child_pt.query(from.start + 10).unwrap().0, start_paddr + 10);\n \n-    let sibling_pt = pt.clone_with(pt.cursor_mut(&(0..MAX_USERSPACE_VADDR)).unwrap());\n+    let sibling_pt = {\n+        let sibling_pt = PageTable::<UserMode>::empty();\n+        let range = 0..MAX_USERSPACE_VADDR;\n+        let mut sibling_cursor = sibling_pt.cursor_mut(&range).unwrap();\n+        let mut parent_cursor = pt.cursor_mut(&range).unwrap();\n+        unsafe { sibling_cursor.copy_from(&mut parent_cursor, range.len(), &mut prot_op) };\n+        sibling_pt\n+    };\n     assert!(sibling_pt.query(from.start + 10).is_none());\n     assert_eq!(child_pt.query(from.start + 10).unwrap().0, start_paddr + 10);\n     drop(pt);\n", "problem_statement": "[RFC] Safety model about the page tables\n# Background\r\n\r\nThis issue discusses the internal APIs of the page table. More specifically, the following two sets of APIs:\r\n - The APIs provided by `RawPageTableNode`/`PageTableNode`\r\n   - Files: [`framework/aster-frame/src/mm/page_table/node.rs`](https://github.com/asterinas/asterinas/blob/main/framework/aster-frame/src/mm/page_table/node.rs)\r\n - The APIs provided by `PageTable`/`Cursor`/`CursorMut`\r\n   - Files: [`framework/aster-frame/src/mm/page_table/mod.rs`](https://github.com/asterinas/asterinas/blob/main/framework/aster-frame/src/mm/page_table/mod.rs) and [`framework/aster-frame/src/mm/page_table/cursor.rs`](https://github.com/asterinas/asterinas/blob/main/framework/aster-frame/src/mm/page_table/cursor.rs)\r\n\r\nThe focus is on what kind of safety guarantees they can provide.\r\n\r\nCurrently, this question is not clearly answered. For example, consider the following API in `PageTableNode`:\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/node.rs#L383-L388\r\n\r\nThis method is marked as unsafe because it can create arbitrary mappings. This is not a valid reason to mark it as unsafe, as the activation of a `RawPageTableNode` is already marked as unsafe, as shown in the following code snippet:\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/node.rs#L112-L124\r\n\r\n_If_ the above reason is considered valid, then _every_ modification method of `PageTableNode` must also be marked as unsafe. This is because a `PageTableNode` does not know its exact position in the page table, so it can be at a critical position (e.g. the kernel text). In such cases, its modification will never be safe in the sense of mapping safety.\r\n\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/node.rs#L372-L373\r\n\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/node.rs#L356-L362\r\n\r\nFortunately, the unsafety of the activation method `RawPageTableNode::activate` should have already captured the mapping safety, so I argue that all other modification methods like `PageTableNode::set_child_untracked` mentioned above should not consider the mapping safety again. However, it should consider the safety of the page tables themselves.\r\n\r\nBut the safety of the page tables themselves still involves a lot of things, like the following:\r\n - **Property 1**: If any PTE points to another page table, it must point to a valid page table.\r\n - **Property 2**: If any PTE points to a physical page, it can point to either a tracked frame or an untracked region of memory.\r\n - **Property 3**: If any PTE points to a physical page and the current page table node can only represent tracked mappings, the PTE must point to a tracked frame.\r\n - **Property 4**: If any PTE points to a physical page and the current page table node can only represent untracked mappings, the PTE must point to an untracked region of memory.\r\n - **Property 5**: If any PTE points to another page table, it must point to a page table that is on the next page level. If the next page level does not exist, the PTE cannot point to a page table.\r\n\r\nThe current design does indeed guarantee **Property 1** and **Property 2**, but the APIs need some revision to make them truly safe. However, it runs into difficulties when dropping the page tables, because the page table nodes do not know whether PTEs point to tracked frames or untracked regions of memory. The API change and the difficulties are described below as **Solution 1**.\r\n\r\nTo address the above difficulties, I think that it is possible to additionally guarantee **Property 3** and **Property 4** through safe APIs of page table nodes. I call this **Solution 2** below.\r\n\r\nI don't think that **Property 5** needs to be guaranteed by `PageTableNode`. The reason is that it can be trivially guaranteed by the page table cursors. The page table cursors maintain a fixed-length array, where each slot can have a page table node at a certain level. It is clear enough, so there is little benefit to enforce these guarantees to the page table nodes.\r\n\r\n# Solution 0\r\n\r\nDo nothing.\r\n\r\n**Pros:**\r\n - No more work to do!\r\n\r\n**Cons:**\r\n - The current APIs are not as good as I would like them to be, and I think they are hard to maintain.\r\n\r\n# Solution 1\r\n\r\nThe current design guarantees **Property 1** and **Property 2**. However, most of the `PageTableNode` APIs cannot be considered safe because they rely on the correctness of the input argument `in_untracked_range` to be memory safe:\r\n\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/node.rs#L267-L268\r\n\r\nFor example, if someone passes `in_untracked_range = false` to `PageTableNode::child`, but the corresponding PTE actually points to an untracked memory range, then the untracked memory range will be cast to an tracked frame. This will cause serve memory safety issues.\r\n\r\nTo solve this problem, it is possible to create a new type called `MaybeTrackedPage`, which can be converted into a tracked frame (via the unsafe `assume_tracked` method) or an untracked region of memory (via the `assume_untracked` method) by the user:\r\n\r\nhttps://github.com/asterinas/asterinas/blob/7f45a1bb29f5bf6d6ddb0d12fdb48dc1ca15852c/framework/aster-frame/src/mm/page_table/node.rs#L253-L268\r\n\r\nThen the `PageTableNode::child` method can be made to return a wrapped type of `MaybeTrackedPage` (the `Child` wrapper handles cases where the PTE is empty or points to another page table):\r\n\r\nhttps://github.com/asterinas/asterinas/blob/7f45a1bb29f5bf6d6ddb0d12fdb48dc1ca15852c/framework/aster-frame/src/mm/page_table/node.rs#L447-L448\r\n\r\nI think this solution works well, _except_ for the annoying `Drop` implementation. Since the page table node has no way of knowing whether PTEs point to tracked frames or untracked regions of memory, it won't know how to drop them if such PTEs are encountered in the `Drop` method. So far it is assumed that only tracked frames can be dropped, as shown in the following code snippet:\r\n\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/node.rs#L536-L540\r\n\r\nBut this assumption can easily be wrong. For example, a page table containing untracked regions of memory can be dropped if a huge page overwrites the PTE on a page table:\r\n\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/node.rs#L474-L476\r\n\r\nIt is possible to work around this problem by adding methods such as `drop_deep_untracked` and `drop_deep_tracked`, which recursively drop all descendants of the current page table node, assuming they contain only tracked frames or untracked regions of memory. Then the `drop` method should not see any PTEs pointing to physical pages.\r\n\r\nhttps://github.com/asterinas/asterinas/blob/7f45a1bb29f5bf6d6ddb0d12fdb48dc1ca15852c/framework/aster-frame/src/mm/page_table/node.rs#L303-L325\r\n\r\nHowever, this solution is not very elegant.\r\n\r\n**Pro:**\r\n - It was implemented in #918, see commits \"Implement `MaybeTracked{,Page,PageRef}`\" and \"Clarify the safety model in `PageTableNode`\".\r\n\r\n**Cons:**\r\n - The dropping implementation is not ideal.\r\n - The cursor (and its users) must be careful about whether the PTE represents tracked frames or untracked regions of memory.\r\n\r\n# Solution 2\r\n\r\nOne possible solution to solve the problem above is to make page table nodes aware whether it contains tracked frames or untracked regions of memory.\r\n\r\nI think it is reasonable to make an additional assumption: a page table node cannot _directly_ contain both PTEs to tracked frames and PTEs to regions of memory. This limits the power of the page table a bit, but is still reasonable. On x86-64, each page table node representing a 1GiB mapping can have either tracked frames or untracked regions of memory, but not both, as 2MiB huge pages, which still seems flexible to me.\r\n\r\nThis information can be recorded in the page metadata, marking each page table as `Tracked` (diretly containing PTEs only to tracked frames), `Untracked` (directly contains PTEs only to untracked regions of memory), or `None` (directly containing no PTEs to physical pages). Then when dropping a page table, it is clear the PTEs can be dropped without problems.\r\n\r\nA simple way to enforce the page metadata is to add assertions at the beginning of methods like `PageTableNode::set_child_frame` and `PageTableNode::set_child_untracked`. Compilers may be smart to check once and update a number of PTEs.\r\n\r\nAlternatively, I think a better solution is to make page table cursors that operate on tracked frames and untracked regions of memory _different modes_ (like the existing `UserMode` and `KernelMode`). This way, whether a cursor operates on tracked frames or untracked regions can be determined at compile time, instead of at runtime as it is now:\r\nhttps://github.com/asterinas/asterinas/blob/c6aa9f9ee860bbdcb8bb3444b630aff9f0ac14af/framework/aster-frame/src/mm/page_table/cursor.rs#L278-L282\r\n\r\nThen the page table cursor and page table node implementation should be much clearer:\r\n```rust\r\nimpl TrackedNode {\r\n    fn set_child(&mut self, idx: usize, frame: Frame);\r\n}\r\n\r\nimpl UntrackedNode {\r\n    fn set_child(&mut self, idx: usize, paddr: Paddr);\r\n}\r\n```\r\n```rust\r\n// `TrackedMode` associates with `TrackedNode`\r\nimpl<M: TrackedMode> Cursor<M> {\r\n    fn map(&mut self, frame: Frame, prop: PageProperty);\r\n}\r\n\r\n// `UntrackedMode` associates with `UntrackedNode`\r\nimpl<M: UntrackedMode> Cursor {\r\n    fn map(&mut self, pa: &Range<Paddr>, prop: PageProperty);\r\n}\r\n```\r\n\r\n**Pros:**\r\n - Improves clarity of cursor and node implementation.\r\n - Addresses the above problem.\r\n\r\n**Cons:**\r\n - Cursor implementation requires more refactoring.\r\n - Cursor may not be as flexible as it is now, but are there use cases where accesses to tracked frames and untracked regions of memory have be mixed in one cursor?\r\n\r\ncc @junyang-zh \n", "hints_text": "I've already checked out your PR #918 addressing issues raised in this RFC, and find it convincing.\r\n\r\nTo sum up, the current inner API designs do have the 2 following major weaknesses:\r\n\r\n - The \"tracked\" and \"untracked\" ranges are all managed by the page table, but the node is agnostic to it to some extent;\r\n - The safety guarantee are not perfectly modeled.\r\n\r\nI need some time carefully think about the solution. And thanks for proposing such a fix quickly.", "created_at": "2024-09-23T14:17:42Z", "version": "0.1", "environment_setup_commit": "ae4ac384713e63232b74915593ebdef680049d31", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "FAIL_TO_FAIL": [], "PASS_TO_FAIL": []}
