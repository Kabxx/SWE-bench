{"repo": "apache/arrow-rs", "pull_number": 6453, "instance_id": "apache__arrow-rs-6453", "issue_numbers": ["6282"], "base_commit": "f41c258246cd4bd9d89228cded9ed54dbd00faff", "patch": "diff --git a/arrow-flight/examples/flight_sql_server.rs b/arrow-flight/examples/flight_sql_server.rs\nindex 81afecf85625..dd3a3943dd95 100644\n--- a/arrow-flight/examples/flight_sql_server.rs\n+++ b/arrow-flight/examples/flight_sql_server.rs\n@@ -19,6 +19,7 @@ use arrow_flight::sql::server::PeekableFlightDataStream;\n use arrow_flight::sql::DoPutPreparedStatementResult;\n use base64::prelude::BASE64_STANDARD;\n use base64::Engine;\n+use core::str;\n use futures::{stream, Stream, TryStreamExt};\n use once_cell::sync::Lazy;\n use prost::Message;\n@@ -168,7 +169,7 @@ impl FlightSqlService for FlightSqlServiceImpl {\n         let bytes = BASE64_STANDARD\n             .decode(base64)\n             .map_err(|e| status!(\"authorization not decodable\", e))?;\n-        let str = String::from_utf8(bytes).map_err(|e| status!(\"authorization not parsable\", e))?;\n+        let str = str::from_utf8(&bytes).map_err(|e| status!(\"authorization not parsable\", e))?;\n         let parts: Vec<_> = str.split(':').collect();\n         let (user, pass) = match parts.as_slice() {\n             [user, pass] => (user, pass),\ndiff --git a/arrow-flight/src/bin/flight_sql_client.rs b/arrow-flight/src/bin/flight_sql_client.rs\nindex c334b95a9a96..8f0618f495bc 100644\n--- a/arrow-flight/src/bin/flight_sql_client.rs\n+++ b/arrow-flight/src/bin/flight_sql_client.rs\n@@ -26,6 +26,7 @@ use arrow_flight::{\n };\n use arrow_schema::Schema;\n use clap::{Parser, Subcommand};\n+use core::str;\n use futures::TryStreamExt;\n use tonic::{\n     metadata::MetadataMap,\ndiff --git a/arrow-flight/src/decode.rs b/arrow-flight/src/decode.rs\nindex 5561f256ce01..7bafc384306b 100644\n--- a/arrow-flight/src/decode.rs\n+++ b/arrow-flight/src/decode.rs\n@@ -388,11 +388,14 @@ struct FlightStreamState {\n /// FlightData and the decoded payload (Schema, RecordBatch), if any\n #[derive(Debug)]\n pub struct DecodedFlightData {\n+    /// The original FlightData message\n     pub inner: FlightData,\n+    /// The decoded payload\n     pub payload: DecodedPayload,\n }\n \n impl DecodedFlightData {\n+    /// Create a new DecodedFlightData with no payload\n     pub fn new_none(inner: FlightData) -> Self {\n         Self {\n             inner,\n@@ -400,6 +403,7 @@ impl DecodedFlightData {\n         }\n     }\n \n+    /// Create a new DecodedFlightData with a [`Schema`] payload\n     pub fn new_schema(inner: FlightData, schema: SchemaRef) -> Self {\n         Self {\n             inner,\n@@ -407,6 +411,7 @@ impl DecodedFlightData {\n         }\n     }\n \n+    /// Create a new [`DecodedFlightData`] with a [`RecordBatch`] payload\n     pub fn new_record_batch(inner: FlightData, batch: RecordBatch) -> Self {\n         Self {\n             inner,\n@@ -414,7 +419,7 @@ impl DecodedFlightData {\n         }\n     }\n \n-    /// return the metadata field of the inner flight data\n+    /// Return the metadata field of the inner flight data\n     pub fn app_metadata(&self) -> Bytes {\n         self.inner.app_metadata.clone()\n     }\ndiff --git a/arrow-flight/src/encode.rs b/arrow-flight/src/encode.rs\nindex 59fa8afd58d5..55bc9240321d 100644\n--- a/arrow-flight/src/encode.rs\n+++ b/arrow-flight/src/encode.rs\n@@ -144,6 +144,7 @@ impl Default for FlightDataEncoderBuilder {\n }\n \n impl FlightDataEncoderBuilder {\n+    /// Create a new [`FlightDataEncoderBuilder`].\n     pub fn new() -> Self {\n         Self::default()\n     }\n@@ -1403,7 +1404,7 @@ mod tests {\n         let input_rows = batch.num_rows();\n \n         let split = split_batch_for_grpc_response(batch.clone(), max_flight_data_size_bytes);\n-        let sizes: Vec<_> = split.iter().map(|batch| batch.num_rows()).collect();\n+        let sizes: Vec<_> = split.iter().map(RecordBatch::num_rows).collect();\n         let output_rows: usize = sizes.iter().sum();\n \n         assert_eq!(sizes, expected_sizes, \"mismatch for {batch:?}\");\ndiff --git a/arrow-flight/src/error.rs b/arrow-flight/src/error.rs\nindex ba979ca9f7a6..499706e1ede7 100644\n--- a/arrow-flight/src/error.rs\n+++ b/arrow-flight/src/error.rs\n@@ -37,6 +37,7 @@ pub enum FlightError {\n }\n \n impl FlightError {\n+    /// Generate a new `FlightError::ProtocolError` variant.\n     pub fn protocol(message: impl Into<String>) -> Self {\n         Self::ProtocolError(message.into())\n     }\n@@ -98,6 +99,7 @@ impl From<FlightError> for tonic::Status {\n     }\n }\n \n+/// Result type for the Apache Arrow Flight crate\n pub type Result<T> = std::result::Result<T, FlightError>;\n \n #[cfg(test)]\ndiff --git a/arrow-flight/src/lib.rs b/arrow-flight/src/lib.rs\nindex 64e3ba01c5bd..9f18416c06ec 100644\n--- a/arrow-flight/src/lib.rs\n+++ b/arrow-flight/src/lib.rs\n@@ -37,6 +37,7 @@\n //!\n //! [Flight SQL]: https://arrow.apache.org/docs/format/FlightSql.html\n #![allow(rustdoc::invalid_html_tags)]\n+#![warn(missing_docs)]\n \n use arrow_ipc::{convert, writer, writer::EncodedData, writer::IpcWriteOptions};\n use arrow_schema::{ArrowError, Schema};\n@@ -52,6 +53,8 @@ type ArrowResult<T> = std::result::Result<T, ArrowError>;\n \n #[allow(clippy::all)]\n mod gen {\n+    // Since this file is auto-generated, we suppress all warnings\n+    #![allow(missing_docs)]\n     include!(\"arrow.flight.protocol.rs\");\n }\n \n@@ -125,6 +128,7 @@ use flight_descriptor::DescriptorType;\n \n /// SchemaAsIpc represents a pairing of a `Schema` with IpcWriteOptions\n pub struct SchemaAsIpc<'a> {\n+    /// Data type representing a schema and its IPC write options\n     pub pair: (&'a Schema, &'a IpcWriteOptions),\n }\n \n@@ -684,6 +688,7 @@ impl PollInfo {\n }\n \n impl<'a> SchemaAsIpc<'a> {\n+    /// Create a new `SchemaAsIpc` from a `Schema` and `IpcWriteOptions`\n     pub fn new(schema: &'a Schema, options: &'a IpcWriteOptions) -> Self {\n         SchemaAsIpc {\n             pair: (schema, options),\ndiff --git a/arrow-flight/src/sql/client.rs b/arrow-flight/src/sql/client.rs\nindex ef52aa27ef50..e45e505b2b61 100644\n--- a/arrow-flight/src/sql/client.rs\n+++ b/arrow-flight/src/sql/client.rs\n@@ -695,9 +695,11 @@ fn flight_error_to_arrow_error(err: FlightError) -> ArrowError {\n     }\n }\n \n-// A polymorphic structure to natively represent different types of data contained in `FlightData`\n+/// A polymorphic structure to natively represent different types of data contained in `FlightData`\n pub enum ArrowFlightData {\n+    /// A record batch\n     RecordBatch(RecordBatch),\n+    /// A schema\n     Schema(Schema),\n }\n \ndiff --git a/arrow-flight/src/sql/metadata/sql_info.rs b/arrow-flight/src/sql/metadata/sql_info.rs\nindex 97304d3c872d..2ea30df7fc2f 100644\n--- a/arrow-flight/src/sql/metadata/sql_info.rs\n+++ b/arrow-flight/src/sql/metadata/sql_info.rs\n@@ -331,7 +331,7 @@ impl SqlInfoUnionBuilder {\n ///\n /// Servers constuct - usually static - [`SqlInfoData`] via the [`SqlInfoDataBuilder`],\n /// and build responses using [`CommandGetSqlInfo::into_builder`]\n-#[derive(Debug, Clone, PartialEq)]\n+#[derive(Debug, Clone, PartialEq, Default)]\n pub struct SqlInfoDataBuilder {\n     /// Use BTreeMap to ensure the values are sorted by value as\n     /// to make output consistent\n@@ -341,17 +341,10 @@ pub struct SqlInfoDataBuilder {\n     infos: BTreeMap<u32, SqlInfoValue>,\n }\n \n-impl Default for SqlInfoDataBuilder {\n-    fn default() -> Self {\n-        Self::new()\n-    }\n-}\n-\n impl SqlInfoDataBuilder {\n+    /// Create a new SQL info builder\n     pub fn new() -> Self {\n-        Self {\n-            infos: BTreeMap::new(),\n-        }\n+        Self::default()\n     }\n \n     /// register the specific sql metadata item\ndiff --git a/arrow-flight/src/sql/metadata/xdbc_info.rs b/arrow-flight/src/sql/metadata/xdbc_info.rs\nindex 2e635d3037bc..485bedaebfb0 100644\n--- a/arrow-flight/src/sql/metadata/xdbc_info.rs\n+++ b/arrow-flight/src/sql/metadata/xdbc_info.rs\n@@ -41,24 +41,43 @@ use crate::sql::{CommandGetXdbcTypeInfo, Nullable, Searchable, XdbcDataType, Xdb\n /// Data structure representing type information for xdbc types.\n #[derive(Debug, Clone, Default)]\n pub struct XdbcTypeInfo {\n+    /// The name of the type\n     pub type_name: String,\n+    /// The data type of the type\n     pub data_type: XdbcDataType,\n+    /// The column size of the type\n     pub column_size: Option<i32>,\n+    /// The prefix of the type\n     pub literal_prefix: Option<String>,\n+    /// The suffix of the type\n     pub literal_suffix: Option<String>,\n+    /// The create parameters of the type\n     pub create_params: Option<Vec<String>>,\n+    /// The nullability of the type\n     pub nullable: Nullable,\n+    /// Whether the type is case sensitive\n     pub case_sensitive: bool,\n+    /// Whether the type is searchable\n     pub searchable: Searchable,\n+    /// Whether the type is unsigned\n     pub unsigned_attribute: Option<bool>,\n+    /// Whether the type has fixed precision and scale\n     pub fixed_prec_scale: bool,\n+    /// Whether the type is auto-incrementing\n     pub auto_increment: Option<bool>,\n+    /// The local type name of the type\n     pub local_type_name: Option<String>,\n+    /// The minimum scale of the type\n     pub minimum_scale: Option<i32>,\n+    /// The maximum scale of the type\n     pub maximum_scale: Option<i32>,\n+    /// The SQL data type of the type\n     pub sql_data_type: XdbcDataType,\n+    /// The optional datetime subcode of the type\n     pub datetime_subcode: Option<XdbcDatetimeSubcode>,\n+    /// The number precision radix of the type\n     pub num_prec_radix: Option<i32>,\n+    /// The interval precision of the type\n     pub interval_precision: Option<i32>,\n }\n \n@@ -93,16 +112,6 @@ impl XdbcTypeInfoData {\n     }\n }\n \n-pub struct XdbcTypeInfoDataBuilder {\n-    infos: Vec<XdbcTypeInfo>,\n-}\n-\n-impl Default for XdbcTypeInfoDataBuilder {\n-    fn default() -> Self {\n-        Self::new()\n-    }\n-}\n-\n /// A builder for [`XdbcTypeInfoData`] which is used to create [`CommandGetXdbcTypeInfo`] responses.\n ///\n /// # Example\n@@ -138,6 +147,16 @@ impl Default for XdbcTypeInfoDataBuilder {\n /// // to access the underlying record batch\n /// let batch = info_list.record_batch(None);\n /// ```\n+pub struct XdbcTypeInfoDataBuilder {\n+    infos: Vec<XdbcTypeInfo>,\n+}\n+\n+impl Default for XdbcTypeInfoDataBuilder {\n+    fn default() -> Self {\n+        Self::new()\n+    }\n+}\n+\n impl XdbcTypeInfoDataBuilder {\n     /// Create a new instance of [`XdbcTypeInfoDataBuilder`].\n     pub fn new() -> Self {\ndiff --git a/arrow-flight/src/sql/mod.rs b/arrow-flight/src/sql/mod.rs\nindex 453f608d353a..94bb96a4f852 100644\n--- a/arrow-flight/src/sql/mod.rs\n+++ b/arrow-flight/src/sql/mod.rs\n@@ -43,9 +43,11 @@ use bytes::Bytes;\n use paste::paste;\n use prost::Message;\n \n+#[allow(clippy::all)]\n mod gen {\n-    #![allow(clippy::all)]\n     #![allow(rustdoc::unportable_markdown)]\n+    // Since this file is auto-generated, we suppress all warnings\n+    #![allow(missing_docs)]\n     include!(\"arrow.flight.protocol.sql.rs\");\n }\n \n@@ -163,7 +165,9 @@ macro_rules! prost_message_ext {\n                 /// ```\n                 #[derive(Clone, Debug, PartialEq)]\n                 pub enum Command {\n-                    $($name($name),)*\n+                    $(\n+                        #[doc = concat!(stringify!($name), \"variant\")]\n+                        $name($name),)*\n \n                     /// Any message that is not any FlightSQL command.\n                     Unknown(Any),\n@@ -297,10 +301,12 @@ pub struct Any {\n }\n \n impl Any {\n+    /// Checks whether the message is of type `M`\n     pub fn is<M: ProstMessageExt>(&self) -> bool {\n         M::type_url() == self.type_url\n     }\n \n+    /// Unpacks the contents of the message if it is of type `M`\n     pub fn unpack<M: ProstMessageExt>(&self) -> Result<Option<M>, ArrowError> {\n         if !self.is::<M>() {\n             return Ok(None);\n@@ -310,6 +316,7 @@ impl Any {\n         Ok(Some(m))\n     }\n \n+    /// Packs a message into an [`Any`] message\n     pub fn pack<M: ProstMessageExt>(message: &M) -> Result<Any, ArrowError> {\n         Ok(message.as_any())\n     }\ndiff --git a/arrow-flight/src/utils.rs b/arrow-flight/src/utils.rs\nindex 37d7ff9e7293..f6129ddfe248 100644\n--- a/arrow-flight/src/utils.rs\n+++ b/arrow-flight/src/utils.rs\n@@ -160,9 +160,12 @@ pub fn batches_to_flight_data(\n         dictionaries.extend(encoded_dictionaries.into_iter().map(Into::into));\n         flight_data.push(encoded_batch.into());\n     }\n-    let mut stream = vec![schema_flight_data];\n+\n+    let mut stream = Vec::with_capacity(1 + dictionaries.len() + flight_data.len());\n+\n+    stream.push(schema_flight_data);\n     stream.extend(dictionaries);\n     stream.extend(flight_data);\n-    let flight_data: Vec<_> = stream.into_iter().collect();\n+    let flight_data = stream;\n     Ok(flight_data)\n }\ndiff --git a/arrow-ipc/src/convert.rs b/arrow-ipc/src/convert.rs\nindex 52c6a0d614d0..eef236529e10 100644\n--- a/arrow-ipc/src/convert.rs\n+++ b/arrow-ipc/src/convert.rs\n@@ -133,6 +133,7 @@ pub fn schema_to_fb(schema: &Schema) -> FlatBufferBuilder<'_> {\n     IpcSchemaEncoder::new().schema_to_fb(schema)\n }\n \n+/// Push a key-value metadata into a FlatBufferBuilder and return [WIPOffset]\n pub fn metadata_to_fb<'a>(\n     fbb: &mut FlatBufferBuilder<'a>,\n     metadata: &HashMap<String, String>,\n@@ -152,7 +153,7 @@ pub fn metadata_to_fb<'a>(\n     fbb.create_vector(&custom_metadata)\n }\n \n-#[deprecated(since = \"54.0.0\", note = \"Use `IpcSchemaConverter`.\")]\n+/// Adds a [Schema] to a flatbuffer and returns the offset\n pub fn schema_to_fb_offset<'a>(\n     fbb: &mut FlatBufferBuilder<'a>,\n     schema: &Schema,\ndiff --git a/arrow-ipc/src/lib.rs b/arrow-ipc/src/lib.rs\nindex 4f35ffb60a9f..dde137153964 100644\n--- a/arrow-ipc/src/lib.rs\n+++ b/arrow-ipc/src/lib.rs\n@@ -19,6 +19,7 @@\n //!\n //! [Arrow IPC Format]: https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc\n \n+#![warn(missing_docs)]\n pub mod convert;\n pub mod reader;\n pub mod writer;\n@@ -31,6 +32,7 @@ mod compression;\n #[allow(clippy::redundant_static_lifetimes)]\n #[allow(clippy::redundant_field_names)]\n #[allow(non_camel_case_types)]\n+#[allow(missing_docs)] // Because this is autogenerated\n pub mod gen;\n \n pub use self::gen::File::*;\ndiff --git a/arrow-ipc/src/writer.rs b/arrow-ipc/src/writer.rs\nindex b5cf20ef337f..f9256b4e8175 100644\n--- a/arrow-ipc/src/writer.rs\n+++ b/arrow-ipc/src/writer.rs\n@@ -60,7 +60,7 @@ pub struct IpcWriteOptions {\n     /// Compression, if desired. Will result in a runtime error\n     /// if the corresponding feature is not enabled\n     batch_compression_type: Option<crate::CompressionType>,\n-    /// Flag indicating whether the writer should preserver the dictionary IDs defined in the\n+    /// Flag indicating whether the writer should preserve the dictionary IDs defined in the\n     /// schema or generate unique dictionary IDs internally during encoding.\n     ///\n     /// Defaults to `true`\n@@ -135,6 +135,8 @@ impl IpcWriteOptions {\n         }\n     }\n \n+    /// Return whether the writer is configured to preserve the dictionary IDs\n+    /// defined in the schema\n     pub fn preserve_dict_id(&self) -> bool {\n         self.preserve_dict_id\n     }\n@@ -200,6 +202,11 @@ impl Default for IpcWriteOptions {\n pub struct IpcDataGenerator {}\n \n impl IpcDataGenerator {\n+    /// Converts a schema to an IPC message along with `dictionary_tracker`\n+    /// and returns it encoded inside [EncodedData] as a flatbuffer\n+    ///\n+    /// Preferred method over [IpcDataGenerator::schema_to_bytes] since it's\n+    /// deprecated since Arrow v54.0.0\n     pub fn schema_to_bytes_with_dictionary_tracker(\n         &self,\n         schema: &Schema,\n@@ -234,6 +241,7 @@ impl IpcDataGenerator {\n         since = \"54.0.0\",\n         note = \"Use `schema_to_bytes_with_dictionary_tracker` instead. This function signature of `schema_to_bytes_with_dictionary_tracker` in the next release.\"\n     )]\n+    /// Converts a schema to an IPC message and returns it encoded inside [EncodedData] as a flatbuffer\n     pub fn schema_to_bytes(&self, schema: &Schema, write_options: &IpcWriteOptions) -> EncodedData {\n         let mut fbb = FlatBufferBuilder::new();\n         let schema = {\n@@ -951,6 +959,7 @@ impl<W: Write> FileWriter<W> {\n         })\n     }\n \n+    /// Adds a key-value pair to the [FileWriter]'s custom metadata\n     pub fn write_metadata(&mut self, key: impl Into<String>, value: impl Into<String>) {\n         self.custom_metadata.insert(key.into(), value.into());\n     }\ndiff --git a/arrow-json/src/writer.rs b/arrow-json/src/writer.rs\nindex 86d2e88d99f0..d973206ccf74 100644\n--- a/arrow-json/src/writer.rs\n+++ b/arrow-json/src/writer.rs\n@@ -397,6 +397,7 @@ where\n \n #[cfg(test)]\n mod tests {\n+    use core::str;\n     use std::fs::{read_to_string, File};\n     use std::io::{BufReader, Seek};\n     use std::sync::Arc;\n@@ -1111,7 +1112,7 @@ mod tests {\n             }\n         }\n \n-        let result = String::from_utf8(buf).unwrap();\n+        let result = str::from_utf8(&buf).unwrap();\n         let expected = read_to_string(test_file).unwrap();\n         for (r, e) in result.lines().zip(expected.lines()) {\n             let mut expected_json = serde_json::from_str::<Value>(e).unwrap();\n@@ -1150,7 +1151,7 @@ mod tests {\n     fn json_writer_empty() {\n         let mut writer = ArrayWriter::new(vec![] as Vec<u8>);\n         writer.finish().unwrap();\n-        assert_eq!(String::from_utf8(writer.into_inner()).unwrap(), \"\");\n+        assert_eq!(str::from_utf8(&writer.into_inner()).unwrap(), \"\");\n     }\n \n     #[test]\n@@ -1279,7 +1280,7 @@ mod tests {\n             writer.write(&batch).unwrap();\n         }\n \n-        let result = String::from_utf8(buf).unwrap();\n+        let result = str::from_utf8(&buf).unwrap();\n         let expected = read_to_string(test_file).unwrap();\n         for (r, e) in result.lines().zip(expected.lines()) {\n             let mut expected_json = serde_json::from_str::<Value>(e).unwrap();\n@@ -1321,7 +1322,7 @@ mod tests {\n             writer.write_batches(&batches).unwrap();\n         }\n \n-        let result = String::from_utf8(buf).unwrap();\n+        let result = str::from_utf8(&buf).unwrap();\n         let expected = read_to_string(test_file).unwrap();\n         // result is eq to 2 same batches\n         let expected = format!(\"{expected}\\n{expected}\");\ndiff --git a/arrow-schema/src/field.rs b/arrow-schema/src/field.rs\nindex fc4852a3d37d..b532ea8616b6 100644\n--- a/arrow-schema/src/field.rs\n+++ b/arrow-schema/src/field.rs\n@@ -610,14 +610,14 @@ mod test {\n     #[test]\n     fn test_new_with_string() {\n         // Fields should allow owned Strings to support reuse\n-        let s = String::from(\"c1\");\n+        let s = \"c1\";\n         Field::new(s, DataType::Int64, false);\n     }\n \n     #[test]\n     fn test_new_dict_with_string() {\n         // Fields should allow owned Strings to support reuse\n-        let s = String::from(\"c1\");\n+        let s = \"c1\";\n         Field::new_dict(s, DataType::Int64, false, 4, false);\n     }\n \ndiff --git a/object_store/src/aws/builder.rs b/object_store/src/aws/builder.rs\nindex 75acb73e56a9..c52c3f8dfbd7 100644\n--- a/object_store/src/aws/builder.rs\n+++ b/object_store/src/aws/builder.rs\n@@ -44,7 +44,6 @@ static DEFAULT_METADATA_ENDPOINT: &str = \"http://169.254.169.254\";\n \n /// A specialized `Error` for object store-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n enum Error {\n     #[snafu(display(\"Missing bucket name\"))]\n     MissingBucketName,\ndiff --git a/object_store/src/aws/client.rs b/object_store/src/aws/client.rs\nindex 6fe4889db176..7034a372e95f 100644\n--- a/object_store/src/aws/client.rs\n+++ b/object_store/src/aws/client.rs\n@@ -65,7 +65,6 @@ const USER_DEFINED_METADATA_HEADER_PREFIX: &str = \"x-amz-meta-\";\n \n /// A specialized `Error` for object store-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n pub(crate) enum Error {\n     #[snafu(display(\"Error performing DeleteObjects request: {}\", source))]\n     DeleteObjectsRequest { source: crate::client::retry::Error },\ndiff --git a/object_store/src/aws/resolve.rs b/object_store/src/aws/resolve.rs\nindex 12c9f26d220b..4c7489316b6c 100644\n--- a/object_store/src/aws/resolve.rs\n+++ b/object_store/src/aws/resolve.rs\n@@ -21,7 +21,6 @@ use snafu::{ensure, OptionExt, ResultExt, Snafu};\n \n /// A specialized `Error` for object store-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n enum Error {\n     #[snafu(display(\"Bucket '{}' not found\", bucket))]\n     BucketNotFound { bucket: String },\ndiff --git a/object_store/src/azure/builder.rs b/object_store/src/azure/builder.rs\nindex 35cedeafc049..1c4589ba1ec6 100644\n--- a/object_store/src/azure/builder.rs\n+++ b/object_store/src/azure/builder.rs\n@@ -46,7 +46,6 @@ const MSI_ENDPOINT_ENV_KEY: &str = \"IDENTITY_ENDPOINT\";\n \n /// A specialized `Error` for Azure builder-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n enum Error {\n     #[snafu(display(\"Unable parse source url. Url: {}, Error: {}\", url, source))]\n     UnableToParseUrl {\ndiff --git a/object_store/src/azure/client.rs b/object_store/src/azure/client.rs\nindex 04990515543a..06d3fb5c8678 100644\n--- a/object_store/src/azure/client.rs\n+++ b/object_store/src/azure/client.rs\n@@ -60,7 +60,6 @@ static TAGS_HEADER: HeaderName = HeaderName::from_static(\"x-ms-tags\");\n \n /// A specialized `Error` for object store-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n pub(crate) enum Error {\n     #[snafu(display(\"Error performing get request {}: {}\", path, source))]\n     GetRequest {\ndiff --git a/object_store/src/client/get.rs b/object_store/src/client/get.rs\nindex 0fef5785c052..ae6a8d9deaae 100644\n--- a/object_store/src/client/get.rs\n+++ b/object_store/src/client/get.rs\n@@ -96,7 +96,6 @@ impl ContentRange {\n \n /// A specialized `Error` for get-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n enum GetResultError {\n     #[snafu(context(false))]\n     Header {\ndiff --git a/object_store/src/lib.rs b/object_store/src/lib.rs\nindex 8820983b2025..a0d83eb0b6dd 100644\n--- a/object_store/src/lib.rs\n+++ b/object_store/src/lib.rs\n@@ -1224,78 +1224,116 @@ pub type Result<T, E = Error> = std::result::Result<T, E>;\n \n /// A specialized `Error` for object store-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n #[non_exhaustive]\n pub enum Error {\n+    /// A fallback error type when no variant matches\n     #[snafu(display(\"Generic {} error: {}\", store, source))]\n     Generic {\n+        /// The store this error originated from\n         store: &'static str,\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error when the object is not found at given location\n     #[snafu(display(\"Object at location {} not found: {}\", path, source))]\n     NotFound {\n+        /// The path to file\n         path: String,\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error for invalid path\n     #[snafu(\n         display(\"Encountered object with invalid path: {}\", source),\n         context(false)\n     )]\n-    InvalidPath { source: path::Error },\n+    InvalidPath {\n+        /// The wrapped error\n+        source: path::Error,\n+    },\n \n+    /// Error when `tokio::spawn` failed\n     #[snafu(display(\"Error joining spawned task: {}\", source), context(false))]\n-    JoinError { source: tokio::task::JoinError },\n+    JoinError {\n+        /// The wrapped error\n+        source: tokio::task::JoinError,\n+    },\n \n+    /// Error when the attempted operation is not supported\n     #[snafu(display(\"Operation not supported: {}\", source))]\n     NotSupported {\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error when the object already exists\n     #[snafu(display(\"Object at location {} already exists: {}\", path, source))]\n     AlreadyExists {\n+        /// The path to the\n         path: String,\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error when the required conditions failed for the operation\n     #[snafu(display(\"Request precondition failure for path {}: {}\", path, source))]\n     Precondition {\n+        /// The path to the file\n         path: String,\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error when the object at the location isn't modified\n     #[snafu(display(\"Object at location {} not modified: {}\", path, source))]\n     NotModified {\n+        /// The path to the file\n         path: String,\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error when an operation is not implemented\n     #[snafu(display(\"Operation not yet implemented.\"))]\n     NotImplemented,\n \n+    /// Error when the used credentials don't have enough permission\n+    /// to perform the requested operation\n     #[snafu(display(\n         \"The operation lacked the necessary privileges to complete for path {}: {}\",\n         path,\n         source\n     ))]\n     PermissionDenied {\n+        /// The path to the file\n         path: String,\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error when the used credentials lack valid authentication\n     #[snafu(display(\n         \"The operation lacked valid authentication credentials for path {}: {}\",\n         path,\n         source\n     ))]\n     Unauthenticated {\n+        /// The path to the file\n         path: String,\n+        /// The wrapped error\n         source: Box<dyn std::error::Error + Send + Sync + 'static>,\n     },\n \n+    /// Error when a configuration key is invalid for the store used\n     #[snafu(display(\"Configuration key: '{}' is not valid for store '{}'.\", key, store))]\n-    UnknownConfigurationKey { store: &'static str, key: String },\n+    UnknownConfigurationKey {\n+        /// The object store used\n+        store: &'static str,\n+        /// The configuration key used\n+        key: String,\n+    },\n }\n \n impl From<Error> for std::io::Error {\ndiff --git a/object_store/src/local.rs b/object_store/src/local.rs\nindex db4b4b05031e..ac10f332d743 100644\n--- a/object_store/src/local.rs\n+++ b/object_store/src/local.rs\n@@ -44,7 +44,6 @@ use crate::{\n \n /// A specialized `Error` for filesystem object store-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n pub(crate) enum Error {\n     #[snafu(display(\"File size for {} did not fit in a usize: {}\", path, source))]\n     FileSizeOverflowedUsize {\ndiff --git a/object_store/src/memory.rs b/object_store/src/memory.rs\nindex 0d72983b0495..b458bdddfbf5 100644\n--- a/object_store/src/memory.rs\n+++ b/object_store/src/memory.rs\n@@ -38,7 +38,6 @@ use crate::{GetOptions, PutPayload};\n \n /// A specialized `Error` for in-memory object store-related errors\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n enum Error {\n     #[snafu(display(\"No data in memory found. Location: {path}\"))]\n     NoDataInMemory { path: String },\ndiff --git a/object_store/src/path/mod.rs b/object_store/src/path/mod.rs\nindex 59e08e2eaba9..4c9bb5f05186 100644\n--- a/object_store/src/path/mod.rs\n+++ b/object_store/src/path/mod.rs\n@@ -36,32 +36,57 @@ pub use parts::{InvalidPart, PathPart};\n \n /// Error returned by [`Path::parse`]\n #[derive(Debug, Snafu)]\n-#[allow(missing_docs)]\n #[non_exhaustive]\n pub enum Error {\n+    /// Error when there's an empty segment between two slashes `/` in the path\n     #[snafu(display(\"Path \\\"{}\\\" contained empty path segment\", path))]\n-    EmptySegment { path: String },\n+    EmptySegment {\n+        /// The source path\n+        path: String,\n+    },\n \n+    /// Error when an invalid segment is encountered in the given path\n     #[snafu(display(\"Error parsing Path \\\"{}\\\": {}\", path, source))]\n-    BadSegment { path: String, source: InvalidPart },\n+    BadSegment {\n+        /// The source path\n+        path: String,\n+        /// The part containing the error\n+        source: InvalidPart,\n+    },\n \n+    /// Error when path cannot be canonicalized\n     #[snafu(display(\"Failed to canonicalize path \\\"{}\\\": {}\", path.display(), source))]\n     Canonicalize {\n+        /// The source path\n         path: std::path::PathBuf,\n+        /// The underlying error\n         source: std::io::Error,\n     },\n \n+    /// Error when the path is not a valid URL\n     #[snafu(display(\"Unable to convert path \\\"{}\\\" to URL\", path.display()))]\n-    InvalidPath { path: std::path::PathBuf },\n+    InvalidPath {\n+        /// The source path\n+        path: std::path::PathBuf,\n+    },\n \n+    /// Error when a path contains non-unicode characters\n     #[snafu(display(\"Path \\\"{}\\\" contained non-unicode characters: {}\", path, source))]\n     NonUnicode {\n+        /// The source path\n         path: String,\n+        /// The underlying `UTF8Error`\n         source: std::str::Utf8Error,\n     },\n \n+    /// Error when the a path doesn't start with given prefix\n     #[snafu(display(\"Path {} does not start with prefix {}\", path, prefix))]\n-    PrefixMismatch { path: String, prefix: String },\n+    PrefixMismatch {\n+        /// The source path\n+        path: String,\n+        /// The mismatched prefix\n+        prefix: String,\n+    },\n }\n \n /// A parsed path representation that can be safely written to object storage\ndiff --git a/parquet/src/compression.rs b/parquet/src/compression.rs\nindex edf675f1302a..ccc060250af4 100644\n--- a/parquet/src/compression.rs\n+++ b/parquet/src/compression.rs\n@@ -298,6 +298,35 @@ mod gzip_codec {\n pub use gzip_codec::*;\n \n /// Represents a valid gzip compression level.\n+///\n+/// Defaults to 6.\n+///\n+/// * 0: least compression\n+/// * 9: most compression (that other software can read)\n+/// * 10: most compression (incompatible with other software, see below)\n+/// #### WARNING:\n+/// Level 10 compression can offer smallest file size,\n+/// but Parquet files created with it will not be readable\n+/// by other \"standard\" paquet readers.\n+///\n+/// Do **NOT** use level 10 if you need other software to\n+/// be able to read the files. Read below for details.\n+///\n+/// ### IMPORTANT:\n+/// There's often confusion about the compression levels in `flate2` vs `arrow`\n+/// as highlighted in issue [#1011](https://github.com/apache/arrow-rs/issues/6282).\n+///\n+/// `flate2` supports two compression backends: `miniz_oxide` and `zlib`.\n+///\n+/// - `zlib` supports levels from 0 to 9.\n+/// - `miniz_oxide` supports levels from 0 to 10.\n+///\n+/// `arrow` uses `flate` with `rust_backend` feature,\n+/// which provides `miniz_oxide` as the backend.\n+/// Therefore 0-10 levels are supported.\n+///\n+/// `flate2` documents this behavior properly with\n+/// [this commit](https://github.com/rust-lang/flate2-rs/pull/430).\n #[derive(Debug, Eq, PartialEq, Hash, Clone, Copy)]\n pub struct GzipLevel(u32);\n \n", "test_patch": "diff --git a/arrow-integration-test/src/lib.rs b/arrow-integration-test/src/lib.rs\nindex d1486fd5a153..ea5b545f2e81 100644\n--- a/arrow-integration-test/src/lib.rs\n+++ b/arrow-integration-test/src/lib.rs\n@@ -21,6 +21,7 @@\n //!\n //! This is not a canonical format, but provides a human-readable way of verifying language implementations\n \n+#![warn(missing_docs)]\n use arrow_buffer::{IntervalDayTime, IntervalMonthDayNano, ScalarBuffer};\n use hex::decode;\n use num::BigInt;\n@@ -49,8 +50,11 @@ pub use schema::*;\n /// See <https://github.com/apache/arrow/blob/master/docs/source/format/Integration.rst#json-test-data-format>\n #[derive(Deserialize, Serialize, Debug)]\n pub struct ArrowJson {\n+    /// The Arrow schema for JSON file\n     pub schema: ArrowJsonSchema,\n+    /// The `RecordBatch`es in the JSON file\n     pub batches: Vec<ArrowJsonBatch>,\n+    /// The dictionaries in the JSON file\n     #[serde(skip_serializing_if = \"Option::is_none\")]\n     pub dictionaries: Option<Vec<ArrowJsonDictionaryBatch>>,\n }\n@@ -60,7 +64,9 @@ pub struct ArrowJson {\n /// Fields are left as JSON `Value` as they vary by `DataType`\n #[derive(Deserialize, Serialize, Debug)]\n pub struct ArrowJsonSchema {\n+    /// An array of JSON fields\n     pub fields: Vec<ArrowJsonField>,\n+    /// An array of metadata key-value pairs\n     #[serde(skip_serializing_if = \"Option::is_none\")]\n     pub metadata: Option<Vec<HashMap<String, String>>>,\n }\n@@ -68,13 +74,20 @@ pub struct ArrowJsonSchema {\n /// Fields are left as JSON `Value` as they vary by `DataType`\n #[derive(Deserialize, Serialize, Debug)]\n pub struct ArrowJsonField {\n+    /// The name of the field\n     pub name: String,\n+    /// The data type of the field,\n+    /// can be any valid JSON value\n     #[serde(rename = \"type\")]\n     pub field_type: Value,\n+    /// Whether the field is nullable\n     pub nullable: bool,\n+    /// The children fields\n     pub children: Vec<ArrowJsonField>,\n+    /// The dictionary for the field\n     #[serde(skip_serializing_if = \"Option::is_none\")]\n     pub dictionary: Option<ArrowJsonFieldDictionary>,\n+    /// The metadata for the field, if any\n     #[serde(skip_serializing_if = \"Option::is_none\")]\n     pub metadata: Option<Value>,\n }\n@@ -115,20 +128,28 @@ impl From<&Field> for ArrowJsonField {\n     }\n }\n \n+/// Represents a dictionary-encoded field in the Arrow JSON format\n #[derive(Deserialize, Serialize, Debug)]\n pub struct ArrowJsonFieldDictionary {\n+    /// A unique identifier for the dictionary\n     pub id: i64,\n+    /// The type of the dictionary index\n     #[serde(rename = \"indexType\")]\n     pub index_type: DictionaryIndexType,\n+    /// Whether the dictionary is ordered\n     #[serde(rename = \"isOrdered\")]\n     pub is_ordered: bool,\n }\n \n+/// Type of an index for a dictionary-encoded field in the Arrow JSON format\n #[derive(Deserialize, Serialize, Debug)]\n pub struct DictionaryIndexType {\n+    /// The name of the dictionary index type\n     pub name: String,\n+    /// Whether the dictionary index type is signed\n     #[serde(rename = \"isSigned\")]\n     pub is_signed: bool,\n+    /// The bit width of the dictionary index type\n     #[serde(rename = \"bitWidth\")]\n     pub bit_width: i64,\n }\n@@ -137,6 +158,7 @@ pub struct DictionaryIndexType {\n #[derive(Deserialize, Serialize, Debug, Clone)]\n pub struct ArrowJsonBatch {\n     count: usize,\n+    /// The columns in the record batch\n     pub columns: Vec<ArrowJsonColumn>,\n }\n \n@@ -144,7 +166,9 @@ pub struct ArrowJsonBatch {\n #[derive(Deserialize, Serialize, Debug, Clone)]\n #[allow(non_snake_case)]\n pub struct ArrowJsonDictionaryBatch {\n+    /// The unique identifier for the dictionary\n     pub id: i64,\n+    /// The data for the dictionary\n     pub data: ArrowJsonBatch,\n }\n \n@@ -152,15 +176,21 @@ pub struct ArrowJsonDictionaryBatch {\n #[derive(Deserialize, Serialize, Clone, Debug)]\n pub struct ArrowJsonColumn {\n     name: String,\n+    /// The number of elements in the column\n     pub count: usize,\n+    /// The validity bitmap to determine null values\n     #[serde(rename = \"VALIDITY\")]\n     pub validity: Option<Vec<u8>>,\n+    /// The data values in the column\n     #[serde(rename = \"DATA\")]\n     pub data: Option<Vec<Value>>,\n+    /// The offsets for variable-sized data types\n     #[serde(rename = \"OFFSET\")]\n     pub offset: Option<Vec<Value>>, // leaving as Value as 64-bit offsets are strings\n+    /// The type id for union types\n     #[serde(rename = \"TYPE_ID\")]\n     pub type_id: Option<Vec<i8>>,\n+    /// The children columns for nested types\n     pub children: Option<Vec<ArrowJsonColumn>>,\n }\n \n@@ -189,6 +219,7 @@ impl ArrowJson {\n         Ok(true)\n     }\n \n+    /// Convert the stored dictionaries to `Vec[RecordBatch]`\n     pub fn get_record_batches(&self) -> Result<Vec<RecordBatch>> {\n         let schema = self.schema.to_arrow_schema()?;\n \n@@ -275,6 +306,7 @@ impl ArrowJsonField {\n     }\n }\n \n+/// Generates a [`RecordBatch`] from an Arrow JSON batch, given a schema\n pub fn record_batch_from_json(\n     schema: &Schema,\n     json_batch: ArrowJsonBatch,\n@@ -877,6 +909,7 @@ pub fn array_from_json(\n     }\n }\n \n+/// Construct a [`DictionaryArray`] from a partially typed JSON column\n pub fn dictionary_array_from_json(\n     field: &Field,\n     json_col: ArrowJsonColumn,\n@@ -965,6 +998,7 @@ fn create_null_buf(json_col: &ArrowJsonColumn) -> Buffer {\n }\n \n impl ArrowJsonBatch {\n+    /// Convert a [`RecordBatch`] to an [`ArrowJsonBatch`]\n     pub fn from_batch(batch: &RecordBatch) -> ArrowJsonBatch {\n         let mut json_batch = ArrowJsonBatch {\n             count: batch.num_rows(),\ndiff --git a/arrow-integration-testing/src/flight_client_scenarios/auth_basic_proto.rs b/arrow-integration-testing/src/flight_client_scenarios/auth_basic_proto.rs\nindex 376e31e15553..34c3c7706df5 100644\n--- a/arrow-integration-testing/src/flight_client_scenarios/auth_basic_proto.rs\n+++ b/arrow-integration-testing/src/flight_client_scenarios/auth_basic_proto.rs\n@@ -15,6 +15,8 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Scenario for testing basic auth.\n+\n use crate::{AUTH_PASSWORD, AUTH_USERNAME};\n \n use arrow_flight::{flight_service_client::FlightServiceClient, BasicAuth, HandshakeRequest};\n@@ -27,6 +29,7 @@ type Result<T = (), E = Error> = std::result::Result<T, E>;\n \n type Client = FlightServiceClient<tonic::transport::Channel>;\n \n+/// Run a scenario that tests basic auth.\n pub async fn run_scenario(host: &str, port: u16) -> Result {\n     let url = format!(\"http://{host}:{port}\");\n     let mut client = FlightServiceClient::connect(url).await?;\ndiff --git a/arrow-integration-testing/src/flight_client_scenarios/integration_test.rs b/arrow-integration-testing/src/flight_client_scenarios/integration_test.rs\nindex 1a6c4e28a76b..c8289ff446a0 100644\n--- a/arrow-integration-testing/src/flight_client_scenarios/integration_test.rs\n+++ b/arrow-integration-testing/src/flight_client_scenarios/integration_test.rs\n@@ -15,6 +15,8 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Integration tests for the Flight client.\n+\n use crate::open_json_file;\n use std::collections::HashMap;\n \n@@ -40,6 +42,7 @@ type Result<T = (), E = Error> = std::result::Result<T, E>;\n \n type Client = FlightServiceClient<tonic::transport::Channel>;\n \n+/// Run a scenario that uploads data to a Flight server and then downloads it back\n pub async fn run_scenario(host: &str, port: u16, path: &str) -> Result {\n     let url = format!(\"http://{host}:{port}\");\n \ndiff --git a/arrow-integration-testing/src/flight_client_scenarios/middleware.rs b/arrow-integration-testing/src/flight_client_scenarios/middleware.rs\nindex 3b71edf446a3..b826ad456055 100644\n--- a/arrow-integration-testing/src/flight_client_scenarios/middleware.rs\n+++ b/arrow-integration-testing/src/flight_client_scenarios/middleware.rs\n@@ -15,6 +15,8 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Scenario for testing middleware.\n+\n use arrow_flight::{\n     flight_descriptor::DescriptorType, flight_service_client::FlightServiceClient, FlightDescriptor,\n };\n@@ -24,6 +26,7 @@ use tonic::{Request, Status};\n type Error = Box<dyn std::error::Error + Send + Sync + 'static>;\n type Result<T = (), E = Error> = std::result::Result<T, E>;\n \n+/// Run a scenario that tests middleware.\n pub async fn run_scenario(host: &str, port: u16) -> Result {\n     let url = format!(\"http://{host}:{port}\");\n     let conn = tonic::transport::Endpoint::new(url)?.connect().await?;\ndiff --git a/arrow-integration-testing/src/flight_client_scenarios.rs b/arrow-integration-testing/src/flight_client_scenarios/mod.rs\nsimilarity index 93%\nrename from arrow-integration-testing/src/flight_client_scenarios.rs\nrename to arrow-integration-testing/src/flight_client_scenarios/mod.rs\nindex 66cced5f4c2e..c5794433764a 100644\n--- a/arrow-integration-testing/src/flight_client_scenarios.rs\n+++ b/arrow-integration-testing/src/flight_client_scenarios/mod.rs\n@@ -15,6 +15,8 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Collection of utilities for testing the Flight client.\n+\n pub mod auth_basic_proto;\n pub mod integration_test;\n pub mod middleware;\ndiff --git a/arrow-integration-testing/src/flight_server_scenarios/auth_basic_proto.rs b/arrow-integration-testing/src/flight_server_scenarios/auth_basic_proto.rs\nindex 20d868953664..5462e5bd674b 100644\n--- a/arrow-integration-testing/src/flight_server_scenarios/auth_basic_proto.rs\n+++ b/arrow-integration-testing/src/flight_server_scenarios/auth_basic_proto.rs\n@@ -15,6 +15,8 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Basic auth test for the Flight server.\n+\n use std::pin::Pin;\n use std::sync::Arc;\n \n@@ -35,6 +37,7 @@ use prost::Message;\n \n use crate::{AUTH_PASSWORD, AUTH_USERNAME};\n \n+/// Run a scenario that tests basic auth.\n pub async fn scenario_setup(port: u16) -> Result {\n     let service = AuthBasicProtoScenarioImpl {\n         username: AUTH_USERNAME.into(),\n@@ -52,6 +55,7 @@ pub async fn scenario_setup(port: u16) -> Result {\n     Ok(())\n }\n \n+/// Scenario for testing basic auth.\n #[derive(Clone)]\n pub struct AuthBasicProtoScenarioImpl {\n     username: Arc<str>,\ndiff --git a/arrow-integration-testing/src/flight_server_scenarios/integration_test.rs b/arrow-integration-testing/src/flight_server_scenarios/integration_test.rs\nindex 76eb9d880199..0c58fae93df5 100644\n--- a/arrow-integration-testing/src/flight_server_scenarios/integration_test.rs\n+++ b/arrow-integration-testing/src/flight_server_scenarios/integration_test.rs\n@@ -15,6 +15,9 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Integration tests for the Flight server.\n+\n+use core::str;\n use std::collections::HashMap;\n use std::pin::Pin;\n use std::sync::Arc;\n@@ -42,6 +45,7 @@ type TonicStream<T> = Pin<Box<dyn Stream<Item = T> + Send + Sync + 'static>>;\n type Error = Box<dyn std::error::Error + Send + Sync + 'static>;\n type Result<T = (), E = Error> = std::result::Result<T, E>;\n \n+/// Run a scenario that tests integration testing.\n pub async fn scenario_setup(port: u16) -> Result {\n     let addr = super::listen_on(port).await?;\n \n@@ -65,6 +69,7 @@ struct IntegrationDataset {\n     chunks: Vec<RecordBatch>,\n }\n \n+/// Flight service implementation for integration testing\n #[derive(Clone, Default)]\n pub struct FlightServiceImpl {\n     server_location: String,\n@@ -100,13 +105,13 @@ impl FlightService for FlightServiceImpl {\n     ) -> Result<Response<Self::DoGetStream>, Status> {\n         let ticket = request.into_inner();\n \n-        let key = String::from_utf8(ticket.ticket.to_vec())\n+        let key = str::from_utf8(&ticket.ticket)\n             .map_err(|e| Status::invalid_argument(format!(\"Invalid ticket: {e:?}\")))?;\n \n         let uploaded_chunks = self.uploaded_chunks.lock().await;\n \n         let flight = uploaded_chunks\n-            .get(&key)\n+            .get(key)\n             .ok_or_else(|| Status::not_found(format!(\"Could not find flight. {key}\")))?;\n \n         let options = arrow::ipc::writer::IpcWriteOptions::default();\ndiff --git a/arrow-integration-testing/src/flight_server_scenarios/middleware.rs b/arrow-integration-testing/src/flight_server_scenarios/middleware.rs\nindex e8d9c521bb99..6685d45dffac 100644\n--- a/arrow-integration-testing/src/flight_server_scenarios/middleware.rs\n+++ b/arrow-integration-testing/src/flight_server_scenarios/middleware.rs\n@@ -15,6 +15,8 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Middleware test for the Flight server.\n+\n use std::pin::Pin;\n \n use arrow_flight::{\n@@ -31,6 +33,7 @@ type TonicStream<T> = Pin<Box<dyn Stream<Item = T> + Send + Sync + 'static>>;\n type Error = Box<dyn std::error::Error + Send + Sync + 'static>;\n type Result<T = (), E = Error> = std::result::Result<T, E>;\n \n+/// Run a scenario that tests middleware.\n pub async fn scenario_setup(port: u16) -> Result {\n     let service = MiddlewareScenarioImpl {};\n     let svc = FlightServiceServer::new(service);\n@@ -44,6 +47,7 @@ pub async fn scenario_setup(port: u16) -> Result {\n     Ok(())\n }\n \n+/// Middleware interceptor for testing\n #[derive(Clone, Default)]\n pub struct MiddlewareScenarioImpl {}\n \ndiff --git a/arrow-integration-testing/src/flight_server_scenarios.rs b/arrow-integration-testing/src/flight_server_scenarios/mod.rs\nsimilarity index 91%\nrename from arrow-integration-testing/src/flight_server_scenarios.rs\nrename to arrow-integration-testing/src/flight_server_scenarios/mod.rs\nindex 48d4e6045684..3833e1c6335c 100644\n--- a/arrow-integration-testing/src/flight_server_scenarios.rs\n+++ b/arrow-integration-testing/src/flight_server_scenarios/mod.rs\n@@ -15,6 +15,7 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+//! Collection of utilities for testing the Flight server.\n use std::net::SocketAddr;\n \n use arrow_flight::{FlightEndpoint, Location, Ticket};\n@@ -27,6 +28,7 @@ pub mod middleware;\n type Error = Box<dyn std::error::Error + Send + Sync + 'static>;\n type Result<T = (), E = Error> = std::result::Result<T, E>;\n \n+/// Listen on a port and return the address\n pub async fn listen_on(port: u16) -> Result<SocketAddr> {\n     let addr: SocketAddr = format!(\"0.0.0.0:{port}\").parse()?;\n \n@@ -36,6 +38,7 @@ pub async fn listen_on(port: u16) -> Result<SocketAddr> {\n     Ok(addr)\n }\n \n+/// Create a FlightEndpoint with a ticket and location\n pub fn endpoint(ticket: &str, location_uri: impl Into<String>) -> FlightEndpoint {\n     FlightEndpoint {\n         ticket: Some(Ticket {\ndiff --git a/arrow-integration-testing/src/lib.rs b/arrow-integration-testing/src/lib.rs\nindex 4ce7b06a1888..ba8e3876c3e3 100644\n--- a/arrow-integration-testing/src/lib.rs\n+++ b/arrow-integration-testing/src/lib.rs\n@@ -17,6 +17,7 @@\n \n //! Common code used in the integration test binaries\n \n+#![warn(missing_docs)]\n use serde_json::Value;\n \n use arrow::array::{Array, StructArray};\n@@ -42,7 +43,9 @@ pub const AUTH_PASSWORD: &str = \"flight\";\n pub mod flight_client_scenarios;\n pub mod flight_server_scenarios;\n \n+/// An Arrow file in JSON format\n pub struct ArrowFile {\n+    /// The schema of the file\n     pub schema: Schema,\n     // we can evolve this into a concrete Arrow type\n     // this is temporarily not being read from\n@@ -51,12 +54,14 @@ pub struct ArrowFile {\n }\n \n impl ArrowFile {\n+    /// Read a single [RecordBatch] from the file\n     pub fn read_batch(&self, batch_num: usize) -> Result<RecordBatch> {\n         let b = self.arrow_json[\"batches\"].get(batch_num).unwrap();\n         let json_batch: ArrowJsonBatch = serde_json::from_value(b.clone()).unwrap();\n         record_batch_from_json(&self.schema, json_batch, Some(&self.dictionaries))\n     }\n \n+    /// Read all [RecordBatch]es from the file\n     pub fn read_batches(&self) -> Result<Vec<RecordBatch>> {\n         self.arrow_json[\"batches\"]\n             .as_array()\n@@ -70,7 +75,7 @@ impl ArrowFile {\n     }\n }\n \n-// Canonicalize the names of map fields in a schema\n+/// Canonicalize the names of map fields in a schema\n pub fn canonicalize_schema(schema: &Schema) -> Schema {\n     let fields = schema\n         .fields()\n@@ -107,6 +112,7 @@ pub fn canonicalize_schema(schema: &Schema) -> Schema {\n     Schema::new(fields).with_metadata(schema.metadata().clone())\n }\n \n+/// Read an Arrow file in JSON format\n pub fn open_json_file(json_name: &str) -> Result<ArrowFile> {\n     let json_file = File::open(json_name)?;\n     let reader = BufReader::new(json_file);\n@@ -157,10 +163,7 @@ pub fn read_gzip_json(version: &str, path: &str) -> ArrowJson {\n     arrow_json\n }\n \n-//\n-// C Data Integration entrypoints\n-//\n-\n+/// C Data Integration entrypoint to export the schema from a JSON file\n fn cdata_integration_export_schema_from_json(\n     c_json_name: *const i8,\n     out: *mut FFI_ArrowSchema,\n@@ -173,6 +176,7 @@ fn cdata_integration_export_schema_from_json(\n     Ok(())\n }\n \n+/// C Data Integration entrypoint to export a batch from a JSON file\n fn cdata_integration_export_batch_from_json(\n     c_json_name: *const i8,\n     batch_num: c_int,\n@@ -263,6 +267,7 @@ pub unsafe extern \"C\" fn arrow_rs_free_error(c_error: *mut i8) {\n     }\n }\n \n+/// A C-ABI for exporting an Arrow schema from a JSON file\n #[no_mangle]\n pub extern \"C\" fn arrow_rs_cdata_integration_export_schema_from_json(\n     c_json_name: *const i8,\n@@ -272,6 +277,7 @@ pub extern \"C\" fn arrow_rs_cdata_integration_export_schema_from_json(\n     result_to_c_error(&r)\n }\n \n+/// A C-ABI to compare an Arrow schema against a JSON file\n #[no_mangle]\n pub extern \"C\" fn arrow_rs_cdata_integration_import_schema_and_compare_to_json(\n     c_json_name: *const i8,\n@@ -281,6 +287,7 @@ pub extern \"C\" fn arrow_rs_cdata_integration_import_schema_and_compare_to_json(\n     result_to_c_error(&r)\n }\n \n+/// A C-ABI for exporting a RecordBatch from a JSON file\n #[no_mangle]\n pub extern \"C\" fn arrow_rs_cdata_integration_export_batch_from_json(\n     c_json_name: *const i8,\n@@ -291,6 +298,7 @@ pub extern \"C\" fn arrow_rs_cdata_integration_export_batch_from_json(\n     result_to_c_error(&r)\n }\n \n+/// A C-ABI to compare a RecordBatch against a JSON file\n #[no_mangle]\n pub extern \"C\" fn arrow_rs_cdata_integration_import_batch_and_compare_to_json(\n     c_json_name: *const i8,\ndiff --git a/arrow/tests/array_cast.rs b/arrow/tests/array_cast.rs\nindex 0fd89cc2bff4..8f86cbeab717 100644\n--- a/arrow/tests/array_cast.rs\n+++ b/arrow/tests/array_cast.rs\n@@ -179,7 +179,7 @@ fn test_can_cast_types() {\n \n /// Create instances of arrays with varying types for cast tests\n fn get_arrays_of_all_types() -> Vec<ArrayRef> {\n-    let tz_name = String::from(\"+08:00\");\n+    let tz_name = \"+08:00\";\n     let binary_data: Vec<&[u8]> = vec![b\"foo\", b\"bar\"];\n     vec![\n         Arc::new(BinaryArray::from(binary_data.clone())),\n@@ -238,9 +238,9 @@ fn get_arrays_of_all_types() -> Vec<ArrayRef> {\n         Arc::new(TimestampMillisecondArray::from(vec![1000, 2000])),\n         Arc::new(TimestampMicrosecondArray::from(vec![1000, 2000])),\n         Arc::new(TimestampNanosecondArray::from(vec![1000, 2000])),\n-        Arc::new(TimestampSecondArray::from(vec![1000, 2000]).with_timezone(tz_name.clone())),\n-        Arc::new(TimestampMillisecondArray::from(vec![1000, 2000]).with_timezone(tz_name.clone())),\n-        Arc::new(TimestampMicrosecondArray::from(vec![1000, 2000]).with_timezone(tz_name.clone())),\n+        Arc::new(TimestampSecondArray::from(vec![1000, 2000]).with_timezone(tz_name)),\n+        Arc::new(TimestampMillisecondArray::from(vec![1000, 2000]).with_timezone(tz_name)),\n+        Arc::new(TimestampMicrosecondArray::from(vec![1000, 2000]).with_timezone(tz_name)),\n         Arc::new(TimestampNanosecondArray::from(vec![1000, 2000]).with_timezone(tz_name)),\n         Arc::new(Date32Array::from(vec![1000, 2000])),\n         Arc::new(Date64Array::from(vec![1000, 2000])),\n", "problem_statement": "What is the highest compression level in gzip?\n**Which part is this question about**\r\nWhat is the highest compression level in gzip?\r\n\r\n\r\n**Describe your question**\r\nI see from other sources, including `flate2`, the highest compression level for gzip is 9 instead of 10. If we pass 10, it should be accepted by parquet but rejected by flate2. Am I getting misunderstanding somewhere?\r\n```rust\r\nimpl CompressionLevel<u32> for GzipLevel {\r\n    const MINIMUM_LEVEL: u32 = 0;\r\n    const MAXIMUM_LEVEL: u32 = 10;\r\n}\r\n```\r\n\n", "hints_text": "It seems flate2 documentation is wrong.\r\n\r\n```rust\r\n/// Returns an integer representing the compression level, typically on a\r\n/// scale of 0-9\r\npub fn level(&self) -> u32 {\r\n    self.0\r\n}\r\n```\r\n\r\nBut internally, inside `DeflateBackend::make` they have `debug_assert!(level.level() <= 10);`. Using compression level up to 10 works fine but panics for bigger values.\n@JakkuSakura After discussing with `flate2` maintainers, I've confirmed that it's actually a documentation issue, but there's a slight caveat as well. `flate2` supports both `miniz` and `zlib` backends, the former enabled by default.\r\n\r\nFor consistency with zlib (which supports up to 9), the documentation states the compression range as `0-9`, but 10 is supported miniz, enabled by default. If the backend is switched to `zlib`, then an attempt to use a compression level 10 will cause a panic.\r\n\r\nI've opened a [pull request in flate2](https://github.com/rust-lang/flate2-rs/pull/427) to explicitly mention this in the docs, so that there's no confusion around this behavior. I hope this resolves your query?\nOne more thing to add here. Parquet (and the entire arrow project) uses `flate2` with `rust_backend` feature enabled. Which uses `miniz` backend, thereby supporting level 10 compression, aka `UBER COMPRESSION`. Flate2 still chooses to call 9 as the `best` level of compression because with 10 we might run into performance issues on the user's device.\r\n\r\nThe PR I created in `flate2` is merged. So the docs should mention this caveat very soon hopefully. But behaviorally speaking, using level = 10 in parquet shouldn't be a problem at all. Discretion is advised when using `flate2` separately.\r\n\r\n@alamb @tustvold what do you think? And should we close this?\nMaybe we can add a note to the arrow documentation with a link to flate2 and close this issuse?\r\n\nSounds like a good idea. I'll make this a part of #37 exercise itself.\n@alamb @tustvold have a look please before I add anything to the docs.\r\n\r\nhttps://github.com/rust-lang/flate2-rs/pull/427#issuecomment-2377460294 and https://github.com/rust-lang/flate2-rs/pull/429\n> @alamb @tustvold have a look please before I add anything to the docs.\r\n> \r\n> [rust-lang/flate2-rs#427 (comment)](https://github.com/rust-lang/flate2-rs/pull/427#issuecomment-2377460294) and [rust-lang/flate2-rs#429](https://github.com/rust-lang/flate2-rs/pull/429)\r\n\r\nI recommend linking to the docs added in https://github.com/rust-lang/flate2-rs/pull/430 -- they are pretty clear to me. Basically we can say the max is 10 but offer the caveat that nothing else will be able to read the parquet files", "created_at": "2024-09-25T04:27:02Z", "version": "53.0", "environment_setup_commit": "f41c258246cd4bd9d89228cded9ed54dbd00faff", "FAIL_TO_PASS": [], "PASS_TO_PASS": [], "FAIL_TO_FAIL": [], "PASS_TO_FAIL": []}
